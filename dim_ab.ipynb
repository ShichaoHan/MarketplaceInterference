{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.special import kl_div\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "import subprocess\n",
    "import random\n",
    "import time \n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_cate(Yt, Jt, Yc, Jc):\n",
    "    \"\"\"\n",
    "    Let nt be the sample size for global treatment, let nc be the sample size for global control.\n",
    "        Yt: outcome of global treatment samples, len nt\n",
    "        Yc: outcome of global control samples, len nc\n",
    "        Jt: indicator function of whether exposed item in specified category for global treatment samples, len nt\n",
    "        Jc: indicator function of whether exposed item in specified category for global control samples, len nc\n",
    "    \"\"\"\n",
    "    nt, nc = len(Yt), len(Yc)\n",
    "    cate = np.mean(Yt * Jt) - np.mean(Yc * Jc)\n",
    "    var = np.var(Yt * Jt) / nt + np.var(Yc * Jc) / nc\n",
    "    stderr = np.sqrt(var)\n",
    "    \"\"\"\n",
    "    95% confidence interval: (cate - 1.96stderr, cate + 1.96stderr)\n",
    "    \"\"\"\n",
    "    return cate, stderr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_est_cate(obs_T, obs_C, Jt, Jc, treated_probability, Q):\n",
    "    n1,n0 = len(obs_T), len(obs_C)\n",
    "    tau1 = np.sum(obs_T * Jt) / (Q*treated_probability)\n",
    "    tau0 = np.sum(obs_C * Jc)/(Q * (1-treated_probability))\n",
    "    estimate = tau1 - tau0\n",
    "    var = (np.sum((obs_T* Jt / treated_probability - estimate) ** 2) + np.sum(( - obs_C * Jc / (1-treated_probability) - estimate) ** 2)) / Q\n",
    "    return estimate, var\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_gradient_subgroup(predict_p_treat, predict_outcome_treat, predict_p_control, predict_outcome_control, J):\n",
    "    # J: dimension K, indicator function of whether item k belongs to the subgroup\n",
    "    Ey1 = np.sum(predict_p_treat * predict_outcome_treat * J, axis=1, keepdims=True)\n",
    "    Ey0 = np.sum(predict_p_control * predict_outcome_control * J, axis=1, keepdims=True)\n",
    "    dHdtheta0 = predict_p_treat * (predict_outcome_treat * J - Ey1) - predict_p_control * (predict_outcome_control * J - Ey0)\n",
    "    dHdtheta0 = dHdtheta0[:, 1:]\n",
    "    dHdtheta1 = predict_p_treat * (predict_outcome_treat * J - Ey1) \n",
    "    dHdmu = (predict_p_treat - predict_p_control) * J\n",
    "    gradient_vector_H = np.concatenate([dHdtheta0, dHdtheta1, dHdmu], axis =1 )\n",
    "    return gradient_vector_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.10'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Helper functions and set-ups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graphing set-up \n",
    "import seaborn as sns\n",
    "x = np.linspace(-4, 4, 100)\n",
    "tencent_blue = (0,0.3215686274509804,0.8509803921568627)\n",
    "tencent_orange = (0.9333333333333333, 0.49411764705882355, 0.2784313725490196)\n",
    "\n",
    "# Calculate y-values for the standard normal density curve\n",
    "y_standard_normal = (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Simultation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modifying the tensor for 3d input \n",
    "class MyModel_multiple(Model):\n",
    "    def __init__(self, k, num_treats):\n",
    "        super(MyModel_multiple, self).__init__()\n",
    "        self.k = k\n",
    "        self.num_treats = num_treats\n",
    "        self.groupNames = ['A'] + ['B' + str(i+1) for i in range(self.num_treats)]\n",
    "        self.baseline_logit = Dense(1, activation = \"linear\")\n",
    "        self.outcome = Dense(1, activation = \"linear\")\n",
    "        self.logit_dense_layer = {} \n",
    "        for g in self.groupNames:\n",
    "            self.logit_dense_layer[g] = Dense(1, activation = \"linear\")\n",
    "        self.softmax = tf.keras.activations.softmax\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        split_structure =  [2] + [1] * self.num_treats + [1]\n",
    "        splitted_elements = tf.split(inputs, split_structure, axis=2)\n",
    "        x1 = splitted_elements[0]\n",
    "        exposure = tf.squeeze(splitted_elements[self.num_treats + 1], axis=-1)\n",
    "        _, K, dim_x = x1.shape\n",
    "        \n",
    "        \n",
    "        ## Step 1: Reshape the input \n",
    "        reshape_x1 = tf.reshape(x1, (-1, dim_x))\n",
    "        \n",
    "        ## Step 2: Score \n",
    "        ### Baseline logit\n",
    "        x1_final = self.baseline_logit(x1)\n",
    "        \n",
    "        ### Uplift\n",
    "        for i in range(self.num_treats):\n",
    "            w_g = splitted_elements[i + 1]\n",
    "            xg_hidden = self.logit_dense_layer['B'+str(i+1)](x1)\n",
    "            x1_final = tf.add(tf.multiply(w_g, xg_hidden), x1_final)\n",
    "            \n",
    "        ## Step 3: Softmax\n",
    "        logit = tf.reshape(x1_final, (-1, self.k))\n",
    "        softmax_p =  self.softmax(logit, axis=-1)\n",
    "\n",
    "        ## Outcome \n",
    "        ypredicts = self.outcome(x1)\n",
    "        ypredicts = tf.squeeze(ypredicts, axis=-1)\n",
    "\n",
    "        y2 = tf.reduce_sum(tf.multiply(exposure, ypredicts), axis = 1, keepdims=True)\n",
    "        res = tf.concat([softmax_p, logit, y2, ypredicts], axis=1)\n",
    "        return res\n",
    "\n",
    "# Define custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "    y1_true, y2_true = tf.split(y_true, [K, 1], axis=1)\n",
    "    _, y1_logit_pred, y2_pred, _= tf.split(y_pred, [K, K, 1, K], axis=1)\n",
    "    loss1 = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y1_true, y1_logit_pred)\n",
    "    loss2 = tf.keras.losses.MeanSquaredError()(y2_true, y2_pred)\n",
    "    return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modifying the tensor for 3d input \n",
    "class MyModel_true():\n",
    "    def __init__(self, k, promo):\n",
    "        self.k = k\n",
    "        self.promo = promo\n",
    "        \n",
    "    \n",
    "    def predict(self, inputs):\n",
    "\n",
    "        X_goodbads, X_utility, W_matrix, exposure_matrix = np.split(inputs, [1,2,3], axis=2)\n",
    "        logit = self.promo * W_matrix * X_goodbads + X_utility\n",
    "        logit = np.squeeze(logit, axis=-1)\n",
    "        softmax_p =  np.exp(logit) / np.sum(np.exp(logit), axis=1, keepdims=True)\n",
    "\n",
    "        ypredicts = np.squeeze(X_utility, axis=-1)\n",
    "\n",
    "        exposure = np.squeeze(exposure_matrix, axis=-1)\n",
    "\n",
    "        y2 = np.sum(exposure * ypredicts, axis = 1, keepdims=True)\n",
    "        res = np.concatenate([softmax_p, logit, y2, ypredicts], axis=1)\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel_random():\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "\n",
    "        X_goodbads, X_utility, W_matrix, exposure_matrix = np.split(inputs, [1,2,3], axis=2)\n",
    "        K = X_utility.shape[1]\n",
    "        logit = np.ones_like(X_utility)\n",
    "        logit = np.squeeze(logit, axis=-1)\n",
    "        softmax_p = np.ones_like(logit) / K\n",
    "        ypredicts = np.squeeze(X_utility, axis=-1)\n",
    "\n",
    "        exposure = np.squeeze(exposure_matrix, axis=-1)\n",
    "\n",
    "        y2 = np.sum(exposure * ypredicts, axis = 1, keepdims=True)\n",
    "        res = np.concatenate([softmax_p, logit, y2, ypredicts], axis=1)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 30 \n",
    "K = 5 \n",
    "Q = 800 \n",
    "uplift_factor = 1.0\n",
    "truth_estimate, truth_stderr = find_ate_ground_truth(J, K, Q, uplift_factor)\n",
    "\n",
    "L = 1\n",
    "\n",
    "M = 30 ## Number of iterations for Hessian matrix estimation \n",
    "n_folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start K = 5, Q = 800, J = 30\n",
      "Epoch 1/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 133.2026\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 130.5829\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 128.0934\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 125.5555\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 123.1797\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 120.7553\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 118.3649\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 116.0495\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 113.8181\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 111.5235\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "Epoch 1/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 118.6665\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 116.2695\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 113.9245\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 111.4983\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 109.2518\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 106.9324\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 104.7751\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 102.5166\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 100.4016\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 98.2501\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "Epoch 1/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 1.1629\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 1.0674\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 1.0009\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.9580\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.9346\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.9218\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.9146\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.9101\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.9072\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.9046\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "finish simulation.\n"
     ]
    }
   ],
   "source": [
    "B = 1\n",
    "epochs = 10\n",
    "np.random.seed(int(time.time() * 1e8 % 1e8))\n",
    "print(\"Start K = {}, Q = {}, J = {}\".format(str(K), str(Q), str(J)))\n",
    "for b in range(B):\n",
    "    (X_utility, X_goodbads, query_matrix, utility_score_matrix, \n",
    "     treatment_dict, utility_score, good_bad_dict) = generate_environment(J, K, Q, uplift_factor)\n",
    "    (query_matrix, X_goodbads, X_utility,W_matrix, exposure_matrix, \n",
    "     outcome_potential, X_logit) = DGP_new_heterogeneous(J, Q, K, uplift_factor, query_matrix, X_goodbads, \n",
    "                                                         X_utility, treat_control_pool = [True, False])\n",
    "    observed_queries_treatment = np.sum(exposure_matrix * W_matrix, axis = 1 )\n",
    "    observed_outcome = np.sum(outcome_potential * exposure_matrix, axis = 1 )\n",
    "    T, C = observed_outcome[observed_queries_treatment == 1] , observed_outcome[observed_queries_treatment == 0]  \n",
    "        \n",
    "    \n",
    "    ## Cross-fitting indices \n",
    "    all_inds = generate_indices(np.array(query_matrix).shape[0], n_folds)\n",
    "\n",
    "    ## Iterate over each fold for cross-validation. \n",
    "    hfuncs_each_fold, hfuncs_each_fold_true,  debias_terms_each_fold = {}, {}, {}\n",
    "\n",
    "    for f in range(n_folds):\n",
    "        f_start, f_end = all_inds[f]\n",
    "        f_size = f_end - f_start\n",
    "        \n",
    "        ## Cross-fitting\n",
    "        X_goodbads_train, X_goodbads_test =  train_test_split(X_goodbads, all_inds, f) \n",
    "        X_utility_train, X_utility_test =  train_test_split(X_utility, all_inds, f)  \n",
    "        W_matrix_train, W_matrix_test = train_test_split(W_matrix, all_inds, f) \n",
    "        exposure_matrix_train, exposure_matrix_test =train_test_split(exposure_matrix, all_inds, f) \n",
    "        observed_outcome_train, observed_outcome_test = train_test_split(observed_outcome, all_inds, f) \n",
    "        \n",
    "        outcome_potential_train, outcome_potential_test = train_test_split(outcome_potential, all_inds, f)  \n",
    "    \n",
    "        inputs_3d_train = np.stack([X_goodbads_train, X_utility_train, W_matrix_train, exposure_matrix_train], axis = -1)\n",
    "        inputs_3d_test = np.stack([X_goodbads_test, X_utility_test, W_matrix_test, exposure_matrix_test], axis = -1)\n",
    "        output_3d_train = np.concatenate([exposure_matrix_train.astype(dtype=float), observed_outcome_train[:, np.newaxis]], axis = 1)\n",
    "\n",
    "        myModelMultiple = MyModel_multiple(K, 1)\n",
    "        myModelMultiple.compile(loss=custom_loss, optimizer=tf.keras.optimizers.legacy.Adam())\n",
    "        myModelMultiple.fit(inputs_3d_train, output_3d_train, epochs=epochs, verbose=True)\n",
    "        myModelMultiple_true = MyModel_true(K, uplift_factor)\n",
    "        # myModelMultiple = MyModel_random()\n",
    "\n",
    "        predict_p_test, _, _, predict_outcome_test = np.split(myModelMultiple.predict(inputs_3d_test), [K, 2*K, 2*K+1], axis=1)\n",
    "\n",
    "        input_3d_test_treat = np.stack([X_goodbads_test, X_utility_test, np.ones_like(W_matrix_test), exposure_matrix_test], axis = -1)\n",
    "        input_3d_test_control = np.stack([X_goodbads_test, X_utility_test, np.zeros_like(W_matrix_test), exposure_matrix_test], axis = -1)\n",
    "        \n",
    "        predict_p_treat, _, _, predict_outcome_treat = np.split(myModelMultiple.predict(input_3d_test_treat), [K, 2*K, 2*K+1], axis=1)\n",
    "        predict_p_control, _, _, predict_outcome_control = np.split(myModelMultiple.predict(input_3d_test_control), [K, 2*K, 2*K+1], axis=1)\n",
    "\n",
    "        predict_p_treat_true, _, _, predict_outcome_treat_true = np.split(myModelMultiple_true.predict(input_3d_test_treat), [K, 2*K, 2*K+1], axis=1)\n",
    "        predict_p_control_true, _, _, predict_outcome_control_true = np.split(myModelMultiple_true.predict(input_3d_test_control), [K, 2*K, 2*K+1], axis=1)\n",
    "        \n",
    "\n",
    "        ## 1. COMPUTE THE GRADIENT OF LOSSS  \n",
    "        gradient_vector_l = compute_loss_gradient(predict_p_test, exposure_matrix_test, W_matrix_test, \n",
    "                                                  predict_outcome_test, observed_outcome_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## 2. COMPUTE  THE GRADIENT OF H FUNCTION\n",
    "        gradient_vector_H = compute_value_gradient(predict_p_treat, predict_outcome_treat, predict_p_control, predict_outcome_control)\n",
    "\n",
    "        \n",
    "        # 3. FIND THE EXPECTATION OF HESSIAN MATRIX \n",
    "        Hessian_all = np.zeros((f_size, (L+2) * K - 1,  (L+2) * K - 1))\n",
    "        for m in range(M):\n",
    "            treat_dict_m = permute_treatment_dict(J, L)\n",
    "            W_matrix_m = []\n",
    "            for each_query in query_matrix[f_start:f_end]:\n",
    "                W_matrix_m.append([treat_dict_m[ind] for ind in each_query])\n",
    "            W_matrix_m = np.array(W_matrix_m)\n",
    "            inputs_m = tf.stack([X_goodbads_test, X_utility_test, W_matrix_m, exposure_matrix_test], axis = -1)\n",
    "            predict_p_m, _, _, _ = np.split(myModelMultiple.predict(inputs_m), [K, 2*K, 2*K+1], axis=1)\n",
    "            Hessian = compute_hessian_instance(W_matrix_m, predict_p_m)\n",
    "            Hessian_all = Hessian_all + Hessian\n",
    "        Hessian_final = Hessian_all / M\n",
    "        \n",
    "        count_finite = 0\n",
    "        debias_term_f = np.zeros(len(Hessian_final))\n",
    "        for i in range(f_size):\n",
    "            if is_invertible(Hessian_final[i]):\n",
    "                try:\n",
    "                    debias_term_f[i] = gradient_vector_H[i]@np.linalg.inv(Hessian_final[i])@gradient_vector_l[i]\n",
    "                    count_finite += 1 \n",
    "                except: \n",
    "                    print(\"Fail for inversion\")\n",
    "\n",
    "\n",
    "        ## END OF FOR LOOP FOR EACH ITERATION OVER CROSS FITTING\n",
    "        hfuncs_each_fold[f] = np.sum(predict_p_treat * predict_outcome_treat, axis=1) - np.sum(predict_p_control * predict_outcome_control, axis=1)\n",
    "        debias_terms_each_fold[f] = debias_term_f\n",
    "        hfuncs_each_fold_true[f] = np.sum(predict_p_treat_true * predict_outcome_treat_true, axis=1) - np.sum(predict_p_control_true * predict_outcome_control_true, axis=1)\n",
    "        \n",
    "        \n",
    "    (debias_point, debias_var, undebias_point, undebias_var) = crossfitted_estimate_var(hfuncs_each_fold, debias_terms_each_fold)\n",
    "    (debias_point_one_fold, debias_var_one_fold, undebias_point_one_fold, undebias_var_one_fold) = onefold_estimate_var(hfuncs_each_fold[0], debias_terms_each_fold[0])\n",
    "    undebias_point_true  = np.mean([np.mean(hfuncs_each_fold[f]) for f in hfuncs_each_fold])\n",
    "    undebias_var_true = np.mean([np.mean((hfuncs_each_fold[f] - undebias_point_true) ** 2) for f in hfuncs_each_fold])\n",
    "    dim_point, dim_var = dim_est(T, C, 0.5, Q)\n",
    "    # path = compose_filename(f\"results1106/new_heterogeneous_synthetic_ab_j{J}q{Q}k{K}_100_{uplift_factor}\", \"csv\")\n",
    "    result_df = pd.DataFrame({\"dim\": [dim_point], \"dim_var\":[dim_var],\n",
    "                              \"debias_point\": [debias_point], \"debias_var\":[debias_var], \n",
    "                              \"undebias_point\": [undebias_point], \"undebias_var\": [undebias_var], \n",
    "                              \"debias_point_one_fold\": [debias_point_one_fold], \"debias_var_one_fold\":[debias_var_one_fold], \n",
    "                              \"undebias_point_one_fold\": [undebias_point_one_fold], \"undebias_var_one_fold\": [undebias_var_one_fold],\n",
    "                              \"undebias_point_true\": [undebias_point_true], \"undebias_var_true\": [undebias_var_true], \n",
    "                              \"J\" : [J], \"Q\": [Q],  \"K\":[K], \n",
    "                              \"truth\": [truth_estimate], \"truth_stderr\": [truth_stderr] })\n",
    "    result_df.to_csv(path)\n",
    "    print(\"finish simulation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim</th>\n",
       "      <th>dim_var</th>\n",
       "      <th>debias_point</th>\n",
       "      <th>debias_var</th>\n",
       "      <th>undebias_point</th>\n",
       "      <th>undebias_var</th>\n",
       "      <th>debias_point_one_fold</th>\n",
       "      <th>debias_var_one_fold</th>\n",
       "      <th>undebias_point_one_fold</th>\n",
       "      <th>undebias_var_one_fold</th>\n",
       "      <th>J</th>\n",
       "      <th>Q</th>\n",
       "      <th>K</th>\n",
       "      <th>truth</th>\n",
       "      <th>truth_stderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.191644</td>\n",
       "      <td>103.210898</td>\n",
       "      <td>-5.526988</td>\n",
       "      <td>208.632419</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>0.080184</td>\n",
       "      <td>-14.437738</td>\n",
       "      <td>485.963244</td>\n",
       "      <td>0.212649</td>\n",
       "      <td>0.03163</td>\n",
       "      <td>30</td>\n",
       "      <td>800</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016609</td>\n",
       "      <td>0.000568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dim     dim_var  debias_point  debias_var  undebias_point  \\\n",
       "0  3.191644  103.210898     -5.526988  208.632419        0.251691   \n",
       "\n",
       "   undebias_var  debias_point_one_fold  debias_var_one_fold  \\\n",
       "0      0.080184             -14.437738           485.963244   \n",
       "\n",
       "   undebias_point_one_fold  undebias_var_one_fold   J    Q  K     truth  \\\n",
       "0                 0.212649                0.03163  30  800  5  0.016609   \n",
       "\n",
       "   truth_stderr  \n",
       "0      0.000568  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
