{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install hnswlib\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import hnswlib\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "random.seed(20230802)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "treatP = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_videos = 29446\n",
    "n_users = 933808\n",
    "\n",
    "## Randomly generate embedding vectors for users and videos \n",
    "\n",
    "# Set the length and size of the embedding vectors\n",
    "embedding_length = 16\n",
    "embedding_size = n_videos\n",
    "# Generate the list of embedding vectors with norm 1\n",
    "embedding_list = []\n",
    "for _ in range(embedding_size):\n",
    "    embedding = np.random.randn(embedding_length).astype(float)\n",
    "    embedding /= np.linalg.norm(embedding)\n",
    "    embedding_list.append(\",\".join([str(elm) for elm in embedding]))\n",
    "finder_emb_pd = pd.DataFrame({\"finderuin\": np.arange(n_videos), \"embedding\":embedding_list})\n",
    "\n",
    "\n",
    "# Set the length and size of the embedding vectors\n",
    "embedding_length = 16\n",
    "embedding_size = n_users\n",
    "# Generate the list of embedding vectors with norm 1\n",
    "embedding_list = []\n",
    "for _ in range(embedding_size):\n",
    "    embedding = np.random.randn(embedding_length).astype(float)\n",
    "    embedding /= np.linalg.norm(embedding)\n",
    "    embedding_list.append(\",\".join([str(elm) for elm in embedding]))\n",
    "uin_emb_pd = pd.DataFrame({\"uin\": np.arange(n_users), \"embedding\":embedding_list})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Helper Functions \n",
    "def find_cosine(A_vec, B_vec):\n",
    "    A = np.array([float(i) for i in A_vec])\n",
    "    B = np.array([float(i) for i in B_vec])\n",
    "    return float(np.dot(A,B)/(np.linalg.norm(A,2)*np.linalg.norm(B,2)))\n",
    "\n",
    "def convert_string_to_vector(s):\n",
    "    res = s.split(\",\")\n",
    "    return np.array(res).astype(float)\n",
    "\n",
    "def find_cosine_string(s1, s2):\n",
    "    return find_cosine(convert_string_to_vector(s1), convert_string_to_vector(s2))\n",
    "\n",
    "def mySoftMax(arr):\n",
    "    num = np.exp(arr)\n",
    "    denom = np.sum(num)\n",
    "    return num/denom\n",
    "\n",
    "\n",
    "def naive_est(res):\n",
    "    treat_res = [elm[0] for elm in res[0]]\n",
    "    control_res = [elm[1] for elm in res[0]]\n",
    "    return np.mean(treat_res) - np.mean(control_res)\n",
    "\n",
    "\n",
    "def dim_est(obs_T, obs_C):\n",
    "    n1,n0 = len(obs_T), len(obs_C)\n",
    "    return np.mean(obs_T) -np.mean(obs_C), np.sqrt(np.var(obs_T)/n1 + np.var(obs_C) / n0)\n",
    "\n",
    "\n",
    "def point_est(all_treat_array, all_control_array):\n",
    "    mus_T, mus_C  = all_treat_array[:, 11:21], all_control_array[:,11:21]\n",
    "    p_T, p_C  = all_treat_array[:, 21:], all_control_array[:,21:]\n",
    "    return np.mean(np.sum((mus_T * (p_T - p_C)), axis = 1 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "tau = 15 \n",
    "class Quota_New:\n",
    "    def __init__(self, quota_id, finderuin, feature,current_time, duration, good_or_not, quota,treat, quota_level=\"\", tag_class1=\"\"):\n",
    "        ## 每一个quota都有一个id，对应的创建者的finderuin，feature（128维的emb feature，string type，用逗号隔开； current_time是创建时间，duration是最多持续时间）\n",
    "        ## good_or_not: 是否为好的quota indicator；quota-总的额度 \n",
    "        self.quota = quota\n",
    "        self.tag_class1 = tag_class1\n",
    "        self.quota_id = quota_id\n",
    "        self.findeurin = finderuin\n",
    "        self.chaotou = 0\n",
    "        self.feature = feature\n",
    "        self.create_ts = current_time\n",
    "        self.end_ts = self.create_ts + duration \n",
    "        self.good = good_or_not\n",
    "        self.treat = treat\n",
    "        \n",
    "        \n",
    "    def consume(self, amount):\n",
    "        self.quota -= amount \n",
    "        if self.quota <0:\n",
    "            self.chaotou += abs(self.quota)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "tau = 15\n",
    "class User:\n",
    "    def __init__(self, uin, feature, treatStatus):\n",
    "        self.uin = uin \n",
    "        self.feature = feature \n",
    "        # Store the information \n",
    "        self.hist = []\n",
    "        self.total_budget = np.random.choice([50,100, 200,300, 400,500], p=[0.4,0.2,0.1,0.1,0.1, 0.1])\n",
    "        self.treat = treatStatus\n",
    "    def watch(self, quota_recommended):\n",
    "        matching_score = find_cosine_string(self.feature, quota_recommended.feature)\n",
    "        bid_est = 43 + 11 * matching_score \n",
    "        bid_est += quota_recommended.good * tau\n",
    "        quota_recommended.consume(bid_est)\n",
    "        self.total_budget -= bid_est \n",
    "        return bid_est\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "         \n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "         \n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment_Smart_Recommendation_tiquan:\n",
    "    def __init__(self, uPool,qPool, initialization_feature_matrix,consideration_set_size = 10, feature_dim =16):\n",
    "        \n",
    "        self.userPool =uPool\n",
    "        self.quotaPool = qPool\n",
    "        self.consideration_set_size = consideration_set_size\n",
    "\n",
    "                        \n",
    "        self.p_all = hnswlib.Index(space = 'cosine', dim = feature_dim)\n",
    "        self.p_all.init_index(max_elements = 4 * len(qPool), ef_construction = 200, M = 16)\n",
    "        \n",
    "        #self.p_all.set_ef(50)\n",
    "        #self.p_good.set_ef(50)\n",
    "\n",
    "        \n",
    "        \n",
    "        qIDs = []\n",
    "        goodQuotaIDs = []\n",
    "        treatQuotaIDs = []\n",
    "        self.treatment_vector = []\n",
    " \n",
    "        for i in range(len(qPool)):\n",
    "            ithQuota = qPool[i]\n",
    "            feature_vector = convert_string_to_vector(ithQuota.feature)\n",
    "            qID = int(ithQuota.quota_id)\n",
    "            self.quotaPool[qID]= ithQuota\n",
    "            qIDs += [qID]\n",
    "            self.p_all.add_items(feature_vector, qID)\n",
    "            if ithQuota.good:\n",
    "                goodQuotaIDs += [qID]\n",
    "            if ithQuota.treat == True:\n",
    "                treatQuotaIDs += [qID]\n",
    "                self.treatment_vector += [1]\n",
    "            else:\n",
    "                self.treatment_vector += [0]\n",
    "        self.treatment_vector = np.array(self.treatment_vector)\n",
    "        self.goodQuotaIDs = goodQuotaIDs\n",
    "        self.treatQuotaIDs = treatQuotaIDs\n",
    "\n",
    "        \n",
    "    def recommend(self, u):\n",
    "        ## Return a quota index in the system that the user will watch and we can observe the result \n",
    "        u_feature_vector = convert_string_to_vector(u.feature)\n",
    "        query_result = self.p_all.knn_query(u_feature_vector,k=self.consideration_set_size)\n",
    "        \n",
    "        candidates_, similarity_scores_ = np.array(query_result[0][0]), np.array(query_result[1][0])\n",
    "        ## Heterogeneity: adding a constant to all the good sellers \n",
    "        good_indicator =  np.array([ind_i in self.goodQuotaIDs for ind_i in candidates_])\n",
    "        treat_indicator = np.array([ind_i in self.treatQuotaIDs for ind_i in candidates_])\n",
    "        \n",
    "        added_similarity = 0.5 * treat_indicator * good_indicator * similarity_scores_\n",
    "        \n",
    "        probs = mySoftMax(similarity_scores_ + added_similarity)\n",
    "        ## Exposure Indicators \n",
    "        expose_nums = np.random.choice([1,1,1,1,1,1,1,1,1,1,1,1,1,2,3,4,5], 1)[0]\n",
    "        exposed_elements = probs.argsort()[-expose_nums:][::-1]\n",
    "        expose_vector = np.array([int(i in exposed_elements) for i in range(len(probs))])\n",
    "        \n",
    "        \n",
    "        ## Consideration set feature \n",
    "        consideration_set_feature = np.array([self.quotaPool[ind_i].feature for ind_i in candidates_])\n",
    "        \n",
    "        return candidates_, probs,similarity_scores_, expose_vector, treat_indicator, good_indicator, u_feature_vector, consideration_set_feature  \n",
    "        \n",
    "        \n",
    "    def recommend_in_parallel(self, u):\n",
    "        ## Return a quota index in the system that the user will watch and we can observe the result \n",
    "        u_feature_vector = convert_string_to_vector(u.feature)\n",
    "        query_result = self.p_all.knn_query(u_feature_vector,k=self.consideration_set_size)\n",
    "        \n",
    "        candidates_, similarity_scores_ = np.array(query_result[0][0]), np.array(query_result[1][0])\n",
    "        ## Heterogeneity: adding a constant to all the good sellers \n",
    "        good_indicator =  np.array([ind_i in self.goodQuotaIDs for ind_i in candidates_])\n",
    "        treat_indicator = np.array([ind_i in self.treatQuotaIDs for ind_i in candidates_])\n",
    "        \n",
    "        added_similarity = 0.05 * treat_indicator * good_indicator\n",
    "        ## added similarity in all treated world \n",
    "        added_similarity_T = 0.5 * good_indicator \n",
    "        ## added similarity in all control world \n",
    "        added_similarity_C = 0\n",
    "        probs = mySoftMax(similarity_scores_ + added_similarity)\n",
    "        probs_T = mySoftMax(similarity_scores_ + added_similarity_T)\n",
    "        probs_C = mySoftMax(similarity_scores_ + added_similarity_C)\n",
    "        ## Exposure Indicators \n",
    "        expose_nums = np.random.choice([1,1,1,1,1,1,1,1,1,1,1,1], 1)[0]\n",
    "        exposed_elements = probs.argsort()[-expose_nums:][::-1]\n",
    "        expose_vector = np.array([int(i in exposed_elements) for i in range(len(probs))])\n",
    "        \n",
    "        ## Exposure Indicators in All Treated World \n",
    "        exposed_elements_T = probs_T.argsort()[-expose_nums:][::-1]\n",
    "        expose_vector_T = np.array([int(i in exposed_elements_T) for i in range(len(probs_T))])\n",
    "        ## Exposure Indicator in All Control World \n",
    "        exposed_elements_C = probs_C.argsort()[-expose_nums:][::-1]\n",
    "        expose_vector_C = np.array([int(i in exposed_elements_C) for i in range(len(probs_C))])\n",
    "\n",
    "        ## Consideration set feature \n",
    "        consideration_set_feature = np.array([self.quotaPool[ind_i].feature for ind_i in candidates_])\n",
    "        \n",
    "        return candidates_, probs,similarity_scores_, expose_vector, treat_indicator, good_indicator, u_feature_vector, consideration_set_feature, expose_vector_T, expose_vector_C, probs_T, probs_C\n",
    "    \n",
    "    def run_experiment(self, T, user_arrival_probability=0.999,verbose = False):\n",
    "        traj = []\n",
    "        ended_quotas = []\n",
    "        quota_arrival_probability = 1 - user_arrival_probability\n",
    "        index_tracker = len(self.quotaPool) + 1\n",
    "        samples = []\n",
    "        \n",
    "        for t in range(T):\n",
    "                ## Recommendation \n",
    "            user_coming_index = np.random.choice(np.arange(len(self.userPool)))\n",
    "            user = self.userPool[user_coming_index]\n",
    "            candidates_, probs,similarity_scores, expose_vector, treat_indicator, good_indicator, u_feat, vu_feats = self.recommend(user)\n",
    "            \n",
    "            rewards = []\n",
    "            for i in range(len(expose_vector)):\n",
    "                if expose_vector[i] == 0:\n",
    "                    rewards += [0]\n",
    "                else:\n",
    "                    recommended_quota = self.quotaPool[candidates_[i]]\n",
    "                    reward = user.watch(recommended_quota)\n",
    "                    rewards += [reward]\n",
    "            samples += [[user_coming_index,rewards, candidates_, probs,similarity_scores, expose_vector, treat_indicator, good_indicator, u_feat, vu_feats]]\n",
    "                        #0                   1            2        3        4                   5               6            7               8         9\n",
    "        return samples\n",
    "    \n",
    "    def run_experiment_parallel_world(self, T,user_arrival_probability=0.999,verbose = False):\n",
    "        traj = []\n",
    "        ended_quotas = []\n",
    "        quota_arrival_probability = 1 - user_arrival_probability\n",
    "        index_tracker = len(self.quotaPool) + 1\n",
    "        samples = []\n",
    "        samples_T,samples_C = [],[]\n",
    "        rewards_seller, rewards_seller_T, rewards_seller_C = np.zeros(len(self.quotaPool)), np.zeros(len(self.quotaPool)), np.zeros(len(self.quotaPool))\n",
    "        for t in range(T):\n",
    "            ## Recommendation \n",
    "            ## Step 1: randomly pick a user from user pool \n",
    "            user_coming_index = np.random.choice(np.arange(len(self.userPool)))\n",
    "            user = self.userPool[user_coming_index]\n",
    "            ## Step 2: recommend some videos to the user; since we know the ground truth - the recommend_in_parallel would also return the \n",
    "            ## quantities as if all the sellers were treated or controlled  \n",
    "            candidates_, probs,similarity_scores, expose_vector, treat_indicator, good_indicator, u_feat, vu_feats, e_T, e_C, p_T, p_C = self.recommend_in_parallel(user)\n",
    "            \n",
    "            ## Step 3: Generate outcomes(rewards), and record the status \n",
    "            rewards = []\n",
    "            rewards_T = []\n",
    "            rewards_C = []\n",
    "            for i in range(len(expose_vector)):\n",
    "                ## World I: Real world, W~Bernoulli(0.5)\n",
    "                if expose_vector[i] == 0:\n",
    "                    rewards += [0]\n",
    "                else:\n",
    "                    recommended_quota = self.quotaPool[candidates_[i]]\n",
    "                    reward = user.watch(recommended_quota)\n",
    "                    rewards += [reward]\n",
    "                    rewards_seller[candidates_[i]] += reward\n",
    "                \n",
    "                ## World II: All treated world , W~Bernoulli(1)\n",
    "                if e_T[i] == 0:\n",
    "                    rewards_T += [0]\n",
    "                else:\n",
    "                    recommended_quota = self.quotaPool[candidates_[i]]\n",
    "                    reward = user.watch(recommended_quota)\n",
    "                    rewards_T += [reward]\n",
    "                    rewards_seller_T[candidates_[i]] += reward\n",
    "                \n",
    "                ## World III: All control world , W~Bernoulli(0)\n",
    "                if e_C[i] == 0:\n",
    "                    rewards_C += [0]\n",
    "                else:\n",
    "                    recommended_quota = self.quotaPool[candidates_[i]]\n",
    "                    reward = user.watch(recommended_quota)\n",
    "                    rewards_C += [reward]\n",
    "                    rewards_seller_C[candidates_[i]] += reward\n",
    "                    \n",
    "            samples += [[user_coming_index,rewards, candidates_, probs,similarity_scores, expose_vector, treat_indicator, good_indicator, u_feat, vu_feats]]\n",
    "                        #0                   1            2        3        4                   5               6            7               8         9\n",
    "            samples_T += [[user_coming_index,rewards_T, candidates_, p_T,similarity_scores, e_T, treat_indicator, good_indicator, u_feat, vu_feats]]\n",
    "            samples_C += [[user_coming_index,rewards_C, candidates_, p_C,similarity_scores, e_C, treat_indicator, good_indicator, u_feat, vu_feats]]\n",
    "        return samples, samples_T, samples_C, rewards_seller, rewards_seller_T, rewards_seller_C \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_dim(res):\n",
    "    rewards_matrix = np.array([elm[1] for elm in res])\n",
    "    \n",
    "    expose_matrix = np.array([elm[5] for elm in res])\n",
    "\n",
    "    treat_matrix = np.array([elm[6] for elm in res])\n",
    "    \n",
    "    treat_samples = rewards_matrix[(expose_matrix * treat_matrix)==1] \n",
    "    control_samples = rewards_matrix[(expose_matrix * (1-treat_matrix))==1] \n",
    "    \n",
    "    n1, n0 = len(treat_samples), len(control_samples)\n",
    "    return np.mean(treat_samples) - np.mean(control_samples), np.sqrt(np.var(treat_samples) / n1 + np.var(control_samples) / n0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_mean(res):\n",
    "    rewards_matrix = np.array([elm[1] for elm in res])\n",
    "    \n",
    "    expose_matrix = np.array([elm[5] for elm in res])\n",
    "\n",
    "    samples = rewards_matrix[expose_matrix==1]\n",
    "    return np.mean(samples), np.var(samples) / len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate elements in the two-sided market \n",
    "quota_size = 1000\n",
    "finder_size = 700\n",
    "user_size=10000\n",
    "user_df_inds = np.arange(len(uin_emb_pd))\n",
    "sample_ind = np.random.choice(user_df_inds, size=user_size, replace=False)\n",
    "goodBads=[True, False, False, False]\n",
    "user_pool = []\n",
    "for i in range(user_size):\n",
    "    user_ind = sample_ind[i]\n",
    "    user_pool += [User(uin_emb_pd.iloc[user_ind,1], uin_emb_pd.iloc[user_ind,1], np.random.choice([False, False]))]\n",
    "#user_size = quota_size * 300\n",
    "finder_sample_ind = np.random.choice(np.arange(len(finder_emb_pd)),size = finder_size, replace = False)\n",
    "quota_pool = []\n",
    "quota_pool_treat = []\n",
    "quota_pool_control = [] \n",
    "for i in range(quota_size):\n",
    "    finder_ind = np.random.choice(finder_sample_ind)\n",
    "    g = np.random.choice(goodBads)\n",
    "    q_new = Quota_New(str(i), finder_emb_pd.iloc[finder_ind,1], finder_emb_pd.iloc[finder_ind,1], 0, 100000, g,  9999999999,np.random.choice([True, False]))\n",
    "    q_new_treat = Quota_New(str(i), finder_emb_pd.iloc[finder_ind,1], finder_emb_pd.iloc[finder_ind,1], 0, 100000, g,  9999999999,np.random.choice([True, True]))\n",
    "    q_new_control = Quota_New(str(i), finder_emb_pd.iloc[finder_ind,1], finder_emb_pd.iloc[finder_ind,1], 0, 100000,g,  9999999999,np.random.choice([False, False]))\n",
    "    quota_pool += [q_new]\n",
    "    quota_pool_treat += [q_new_treat]\n",
    "    quota_pool_control += [q_new_control]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create an environment and run some recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create expt environment\n",
    "expt_717 = Environment_Smart_Recommendation_tiquan(user_pool, quota_pool, np.array([convert_string_to_vector(elm) for elm in finder_emb_pd.iloc[finder_sample_ind,1]]))\n",
    "res_717_mixed, res_717_T, res_717_C, rew_seller, rew_seller_T, rew_seller_C =  expt_717.run_experiment_parallel_world(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Process the data for further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct outcomes to the model \n",
    "is_selected_indicator = tf.convert_to_tensor(np.array([elm[5] for elm in res_717_mixed]), dtype = float)\n",
    "reward_array =np.array([elm[5].T@elm[1] for elm in res_717_mixed]).reshape(len(res_717_mixed),1)\n",
    "\n",
    "outcome_tensor = tf.convert_to_tensor(reward_array, dtype = float)\n",
    "# Expsoure indicator and outcomt; can be seen as the label in model fitting \n",
    "\n",
    "exposure_indicator_outcome = tf.concat([is_selected_indicator, outcome_tensor,is_selected_indicator], axis = 1)\n",
    "\n",
    "treatTable = np.array([elm[6] for elm in res_717_mixed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Get treatment indicator matrix\n",
    "w_dict = {}\n",
    "groupNames = [0,1]\n",
    "for v in groupNames:\n",
    "    w_dict[v] = tf.convert_to_tensor(treatTable == v, dtype = float)\n",
    "\n",
    "w_all_treat = tf.convert_to_tensor(np.array([[1] * k for _ in range(treatTable.shape[0])],dtype='float32'))\n",
    "w_all_control = tf.convert_to_tensor(np.array([[0] * k for _ in range(treatTable.shape[0])],dtype='float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposed_score = np.array([elm[4].T@elm[5] for elm in res_717_mixed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposed_good = np.array([elm[7].T@elm[5] for elm in res_717_mixed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposed_rewards =  np.array([np.array(elm[1]).T@elm[5] for elm in res_717_mixed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "X = np.array([exposed_score, exposed_good]).T\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(exposed_rewards, X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 53.99999936, -10.9999994 ,  14.99999995])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outcomes_input =  np.array([results.predict(sm.add_constant(np.array([elm[4], elm[7]]).T)) for elm in res_717_mixed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outcomes_input_tensor = tf.convert_to_tensor(predicted_outcomes_input, dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct inputs to the model \n",
    "ith_treat= 1\n",
    "inputs_scores = tf.convert_to_tensor( np.array([elm[4] for elm in res_717_mixed]),dtype = float)\n",
    "inputs_good = tf.convert_to_tensor( np.array([elm[7] for elm in res_717_mixed]),dtype = float)\n",
    "inputs_3d = tf.stack([inputs_scores]+[inputs_good] + [w_dict[v] for v in groupNames  if v > 0] + [predicted_outcomes_input_tensor] , axis = 2)\n",
    "\n",
    "inputs_all_treat_3d = tf.stack([inputs_scores]+[inputs_good] + [w_all_treat if v== ith_treat else w_all_control for v in groupNames if v>0] +[predicted_outcomes_input_tensor], axis = 2)\n",
    "inputs_all_control_3d = tf.stack([inputs_scores]+[inputs_good] + [w_all_control for v in groupNames if v>0] +[predicted_outcomes_input_tensor], axis = 2)\n",
    "inputs_all_treat_3d_test = inputs_all_treat_3d[int(split_index):,:] \n",
    "inputs_all_control_3d_test = inputs_all_control_3d[int(split_index):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_3d = tf.stack([inputs_scores]+[inputs_good] + [w_dict[v] for v in groupNames  if v > 0]  , axis = 2)\n",
    "\n",
    "# inputs_all_treat_3d = tf.stack([inputs_scores]+[inputs_good] + [w_all_treat if v== ith_treat else w_all_control for v in groupNames if v>0] , axis = 2)\n",
    "# inputs_all_control_3d = tf.stack([inputs_scores]+[inputs_good] + [w_all_control for v in groupNames if v>0] , axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modifying the tensor for 3d input \n",
    "class MyModel_multiple_simple_layers_0911(Model):\n",
    "    def __init__(self, k, num_treats,predictY=False):\n",
    "        super(MyModel_multiple_simple_layers_0911, self).__init__()\n",
    "        self.k = k\n",
    "        self.num_treats = num_treats\n",
    "        self.groupNames = ['A'] + ['B' + str(i+1) for i in range(self.num_treats)]\n",
    "        \n",
    "        self.baseline_logit = Dense(1, activation = \"relu\")\n",
    "        self.logit_dense_layer = {} \n",
    "        for g in self.groupNames:\n",
    "            self.logit_dense_layer[g] = Dense(1, activation = \"relu\")\n",
    "        self.common_hidden = Dense(5, activation = \"relu\")\n",
    "        self.softmax = Dense(k, activation='softmax')\n",
    "        \n",
    "        self.predictY = predictY\n",
    "        self.d5 = Dense(1, activation = \"relu\")\n",
    "        self.doutcome = Dense(1)\n",
    "        \n",
    "        self.d_onehot = tf.keras.layers.Lambda(lambda x: tf.one_hot(tf.argmax(x, axis=-1), k))\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        split_structure =  [2] + [1] * self.num_treats + [1]\n",
    "        splitted_elements = tf.split(inputs, split_structure, axis=2)\n",
    "        x1 = splitted_elements[0]\n",
    "        ypredicts = splitted_elements[-1]\n",
    "        \n",
    "        print(ypredicts.shape, \" shape of the prediction\")\n",
    "        \n",
    "        \n",
    "        ## Step 1: Reshape the input \n",
    "        \n",
    "        reshape_x1 = tf.reshape(x1, (-1, x1.shape[-1]))\n",
    "        \n",
    "        ## Step 2: a common hidden layer\n",
    "        x1_common_hidden = self.common_hidden(reshape_x1)\n",
    "        \n",
    "        #x1_common_hidden_3d = tf.reshape(x1_common_hidden, [x1.shape[0],self.k, x1_common_hidden.shape[1]])\n",
    "        x1_common_hidden_3d = tf.reshape(x1_common_hidden,[-1, k, 5])\n",
    "        \n",
    "        ## Baseline logit\n",
    "        x1_final = self.baseline_logit(x1_common_hidden_3d)\n",
    "       # Create a mask of ones with the same shape as x\n",
    "          # Create a mask of ones with the same shape as x\n",
    "        mask = tf.ones_like(x1_final)\n",
    "\n",
    "        # Get the dynamic shape of x\n",
    "        x_dynamic_shape = tf.shape(x1_final)\n",
    "\n",
    "        # Create indices for the first column in the second dimension\n",
    "        indices = tf.stack([tf.range(x_dynamic_shape[0]), tf.zeros(x_dynamic_shape[0], dtype=tf.int32), tf.zeros(x_dynamic_shape[0], dtype=tf.int32)], axis=1)\n",
    "\n",
    "        # Create updates (all zeros)\n",
    "        updates = tf.zeros(x_dynamic_shape[0])\n",
    "\n",
    "        # Use tf.tensor_scatter_nd_update to apply the updates\n",
    "        mask = tf.tensor_scatter_nd_update(mask, indices, updates)\n",
    "\n",
    "        # Apply the mask to x\n",
    "        x1_final = x1_final * mask\n",
    "\n",
    "        ## Step 3: logit model\n",
    "        for i in range(self.num_treats):\n",
    "            print(\"Here\")\n",
    "            w_g = splitted_elements[i + 1]\n",
    "            xg_hidden = self.logit_dense_layer['B'+str(i+1)](x1_common_hidden_3d)\n",
    "            x1_final = tf.add(tf.multiply(w_g, xg_hidden), x1_final)\n",
    "            \n",
    "        ## Step 4: Softmax\n",
    "        reshaped_data = tf.squeeze(x1_final, axis=-1)\n",
    "        softmax_p =  self.softmax(reshaped_data)\n",
    "\n",
    "\n",
    "\n",
    "        y1 = self.d_onehot(softmax_p)\n",
    "        \n",
    "        if self.predictY:\n",
    "            x5 = self.d5(x1)\n",
    "            x5 = tf.squeeze(x5, axis=-1)\n",
    "            pairwise_outcome = x5\n",
    "            y2 = tf.reduce_sum(tf.multiply(softmax_p, x5), axis = 1 )\n",
    "            y2 = tf.expand_dims(y2, axis=-1)\n",
    "        else:\n",
    "            \n",
    "            ypredicts = splitted_elements[-1]\n",
    "            print(ypredicts.shape, \" predict shape\")\n",
    "            ypredicts = tf.squeeze(ypredicts, axis=-1)\n",
    "            print(\"ypredicts shape\", ypredicts.shape)\n",
    "            pairwise_outcome = ypredicts\n",
    "            y2 = tf.reduce_sum(tf.multiply(softmax_p, ypredicts), axis = 1 )\n",
    "            y2 = tf.expand_dims(y2, axis=-1)\n",
    "            \n",
    "        # ## One hot vector  \n",
    "        # y1 = softmax_p\n",
    "        # x5 = self.d5(ypredicts)\n",
    "        # print(\"softmax shape\", y1.shape)\n",
    "        \n",
    "        # ## Outcome \n",
    "    \n",
    "        # y2 = self.doutcome(tf.multiply(y1, x5))\n",
    "        \n",
    "\n",
    "\n",
    "        res = tf.concat([softmax_p,y2,pairwise_outcome], axis=1)\n",
    "        print(res.shape, \" result shape\")\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "\n",
    "    y1_true, y2_true,_ = tf.split(y_true, [k, 1,k], axis=1)\n",
    "    y1_pred, y2_pred,_ = tf.split(y_pred, [k, 1,k], axis=1)\n",
    "\n",
    "\n",
    "    loss1 = tf.keras.losses.BinaryCrossentropy()(y1_true, y1_pred)\n",
    "    #loss2 = tf.keras.losses.MeanSquaredError()(y2_true, y2_pred)\n",
    "    return loss1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit choice probability model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "## Start training the model and inference\n",
    "\n",
    "\n",
    "# Get the total number of rows in the tensor\n",
    "num_rows =inputs_3d.shape[0]\n",
    "\n",
    "# Calculate the split index based on the desired percentages\n",
    "split_index = tf.constant(int(num_rows * 0.2))\n",
    "\n",
    "# Split the tensor into training and test sets\n",
    "inputs_3d_train_set, inputs_3d_test_set = tf.split(inputs_3d, [split_index, num_rows - split_index], axis=0)\n",
    "exposure_indicator_outcome_train_set, exposure_indicator_outcome_test_set =  tf.split(exposure_indicator_outcome, [split_index, num_rows - split_index], axis=0)\n",
    "\n",
    "\n",
    "# myModelMultiple = MyModel_multiple_simple_layers_0911(k, 1, predictY= False)\n",
    "# myModelMultiple.compile(loss=custom_loss,optimizer=tf.keras.optimizers.legacy.Adam())\n",
    "# myModelMultiple.fit(inputs_3d_train_set, exposure_indicator_outcome_train_set,epochs=10,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "(None, 10, 1)  shape of the prediction\n",
      "Here\n",
      "(None, 10, 1)  predict shape\n",
      "ypredicts shape (None, 10)\n",
      "(None, 21)  result shape\n",
      "(None, 10, 1)  shape of the prediction\n",
      "Here\n",
      "(None, 10, 1)  predict shape\n",
      "ypredicts shape (None, 10)\n",
      "(None, 21)  result shape\n",
      "63/63 [==============================] - 0s 685us/step - loss: 0.3181\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 527us/step - loss: 0.2929\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 909us/step - loss: 0.2556\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 467us/step - loss: 0.2374\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 453us/step - loss: 0.2317\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 448us/step - loss: 0.2276\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 452us/step - loss: 0.2238\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 440us/step - loss: 0.2196\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 480us/step - loss: 0.2145\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 442us/step - loss: 0.2081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2f1a5a7d0>"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "## Start training the model and inference\n",
    "\n",
    "\n",
    "# Get the total number of rows in the tensor\n",
    "num_rows =inputs_3d.shape[0]\n",
    "\n",
    "# Calculate the split index based on the desired percentages\n",
    "split_index = tf.constant(int(num_rows * 0.2))\n",
    "\n",
    "# Split the tensor into training and test sets\n",
    "inputs_3d_train_set, inputs_3d_test_set = tf.split(inputs_3d, [split_index, num_rows - split_index], axis=0)\n",
    "exposure_indicator_outcome_train_set, exposure_indicator_outcome_test_set =  tf.split(exposure_indicator_outcome, [split_index, num_rows - split_index], axis=0)\n",
    "\n",
    "\n",
    "myModelMultiple = MyModel_multiple_simple_layers_0911(k, 1, predictY= False)\n",
    "myModelMultiple.compile(loss=custom_loss,optimizer=tf.keras.optimizers.legacy.Adam())\n",
    "myModelMultiple.fit(inputs_3d_train_set, exposure_indicator_outcome_train_set,epochs=10,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outcomes_input_tensor_test = predicted_outcomes_input_tensor[int(split_index):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 1)  shape of the prediction\n",
      "Here\n",
      "(32, 10, 1)  predict shape\n",
      "ypredicts shape (32, 10)\n",
      "(32, 21)  result shape\n",
      "250/250 [==============================] - 0s 357us/step\n",
      "250/250 [==============================] - 0s 341us/step\n",
      "250/250 [==============================] - 0s 475us/step\n",
      "250/250 [==============================] - 0s 353us/step\n",
      "250/250 [==============================] - 0s 339us/step\n",
      "250/250 [==============================] - 0s 336us/step\n",
      "250/250 [==============================] - 0s 335us/step\n",
      "250/250 [==============================] - 0s 340us/step\n",
      "250/250 [==============================] - 0s 339us/step\n",
      "250/250 [==============================] - 0s 337us/step\n",
      "250/250 [==============================] - 0s 345us/step\n",
      "250/250 [==============================] - 0s 341us/step\n",
      "250/250 [==============================] - 0s 348us/step\n",
      "250/250 [==============================] - 0s 352us/step\n",
      "250/250 [==============================] - 0s 343us/step\n",
      "250/250 [==============================] - 0s 336us/step\n",
      "250/250 [==============================] - 0s 331us/step\n",
      "250/250 [==============================] - 0s 331us/step\n",
      "250/250 [==============================] - 0s 331us/step\n",
      "250/250 [==============================] - 0s 340us/step\n",
      "250/250 [==============================] - 0s 331us/step\n",
      "250/250 [==============================] - 0s 331us/step\n",
      "250/250 [==============================] - 0s 333us/step\n",
      "250/250 [==============================] - 0s 485us/step\n",
      "250/250 [==============================] - 0s 336us/step\n",
      "250/250 [==============================] - 0s 344us/step\n",
      "250/250 [==============================] - 0s 376us/step\n",
      "250/250 [==============================] - 0s 343us/step\n",
      "250/250 [==============================] - 0s 400us/step\n",
      "250/250 [==============================] - 0s 385us/step\n",
      "250/250 [==============================] - 0s 421us/step\n",
      "250/250 [==============================] - 0s 343us/step\n",
      "250/250 [==============================] - 0s 337us/step\n",
      "250/250 [==============================] - 0s 471us/step\n",
      "250/250 [==============================] - 0s 331us/step\n",
      "250/250 [==============================] - 0s 334us/step\n",
      "250/250 [==============================] - 0s 345us/step\n",
      "250/250 [==============================] - 0s 338us/step\n",
      "250/250 [==============================] - 0s 336us/step\n",
      "250/250 [==============================] - 0s 338us/step\n",
      "250/250 [==============================] - 0s 342us/step\n",
      "250/250 [==============================] - 0s 337us/step\n",
      "250/250 [==============================] - 0s 330us/step\n",
      "250/250 [==============================] - 0s 335us/step\n",
      "250/250 [==============================] - 0s 470us/step\n",
      "250/250 [==============================] - 0s 335us/step\n",
      "250/250 [==============================] - 0s 340us/step\n",
      "250/250 [==============================] - 0s 334us/step\n",
      "250/250 [==============================] - 0s 336us/step\n",
      "250/250 [==============================] - 0s 337us/step\n",
      "250/250 [==============================] - 0s 333us/step\n",
      "250/250 [==============================] - 0s 337us/step\n",
      "250/250 [==============================] - 0s 334us/step\n",
      "250/250 [==============================] - 0s 353us/step\n",
      "250/250 [==============================] - 0s 463us/step\n",
      "250/250 [==============================] - 0s 341us/step\n",
      "250/250 [==============================] - 0s 338us/step\n",
      "250/250 [==============================] - 0s 343us/step\n",
      "250/250 [==============================] - 0s 328us/step\n",
      "250/250 [==============================] - 0s 331us/step\n",
      "250/250 [==============================] - 0s 337us/step\n",
      "250/250 [==============================] - 0s 339us/step\n",
      "250/250 [==============================] - 0s 339us/step\n",
      "250/250 [==============================] - 0s 344us/step\n",
      "250/250 [==============================] - 0s 336us/step\n",
      "250/250 [==============================] - 0s 339us/step\n",
      "250/250 [==============================] - 0s 428us/step\n",
      "250/250 [==============================] - 0s 336us/step\n",
      "250/250 [==============================] - 0s 337us/step\n",
      "250/250 [==============================] - 0s 335us/step\n",
      "250/250 [==============================] - 0s 338us/step\n",
      "250/250 [==============================] - 0s 335us/step\n",
      "250/250 [==============================] - 0s 335us/step\n",
      "250/250 [==============================] - 0s 339us/step\n",
      "250/250 [==============================] - 0s 338us/step\n",
      "250/250 [==============================] - 0s 337us/step\n",
      "250/250 [==============================] - 0s 336us/step\n",
      "250/250 [==============================] - 0s 345us/step\n",
      "250/250 [==============================] - 0s 338us/step\n",
      "250/250 [==============================] - 0s 336us/step\n",
      "250/250 [==============================] - 0s 336us/step\n",
      "250/250 [==============================] - 0s 334us/step\n",
      "250/250 [==============================] - 0s 331us/step\n",
      "250/250 [==============================] - 0s 340us/step\n",
      "250/250 [==============================] - 0s 338us/step\n",
      "250/250 [==============================] - 0s 338us/step\n",
      "250/250 [==============================] - 0s 338us/step\n",
      "250/250 [==============================] - 0s 457us/step\n",
      "250/250 [==============================] - 0s 333us/step\n",
      "250/250 [==============================] - 0s 376us/step\n",
      "250/250 [==============================] - 0s 332us/step\n",
      "250/250 [==============================] - 0s 332us/step\n",
      "250/250 [==============================] - 0s 340us/step\n",
      "250/250 [==============================] - 0s 332us/step\n",
      "250/250 [==============================] - 0s 334us/step\n",
      "250/250 [==============================] - 0s 334us/step\n",
      "250/250 [==============================] - 0s 334us/step\n",
      "250/250 [==============================] - 0s 460us/step\n",
      "250/250 [==============================] - 0s 332us/step\n",
      "250/250 [==============================] - 0s 336us/step\n"
     ]
    }
   ],
   "source": [
    "## Monte Carlo estimation of expected exposure probability\n",
    "##  (over the randomness of treatment assignment among the videos in the consideration set)\n",
    "M = 100\n",
    "exposure_indicator_array_test = np.array([elm[5] for elm in res_717_mixed])[int(split_index):,:]\n",
    "montecarlo_expected_probability_test = np.zeros(exposure_indicator_array[int(split_index):,:].shape)\n",
    "for m in range(M):\n",
    "    w_m = np.random.choice([0] + [i + 1 for i in range(len(groupNames)-1)], size = w_all_treat[int(split_index):,:].shape, replace=True)\n",
    "    input_list = [inputs_scores_test,inputs_good_test] \n",
    "    for i in range(len(groupNames)-1):\n",
    "        w_matrix = 1.0 * (w_m == i)\n",
    "        w_m_tensor = tf.convert_to_tensor(w_matrix, dtype='float32')\n",
    "        input_list = input_list + [w_m_tensor] + [predicted_outcomes_input_tensor_test]\n",
    "    inputs_m = tf.stack(input_list, axis = 2)\n",
    "    model_pred_m = np.array(myModelMultiple.predict(inputs_m))[:,:k]\n",
    "    montecarlo_expected_probability_test = montecarlo_expected_probability_test + model_pred_m\n",
    "    \n",
    "montecarlo_expected_probability_test = montecarlo_expected_probability_test / M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/250 [..............................] - ETA: 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 378us/step\n",
      "250/250 [==============================] - 0s 340us/step\n"
     ]
    }
   ],
   "source": [
    "all_treat_array, all_control_array =  myModelMultiple.predict(inputs_all_treat_3d_test),myModelMultiple.predict(inputs_all_control_3d_test)\n",
    "mus_T, mus_C = all_treat_array[:, -k:],all_control_array[:,-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_T, p_C  = all_treat_array[:, :k], all_control_array[:,:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_indicator_array = 1 * (treatTable == ith_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv1,pv0 = np.sum(exposure_indicator_array[int(split_index):,:] * p_T, axis = 1), np.sum(exposure_indicator_array[int(split_index):,:] * p_C, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_given_uvw = p_T * treatment_indicator_array[int(split_index):,:] + p_C * (1 - treatment_indicator_array[int(split_index):,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_array = np.array([elm[1] for elm in res_717_mixed])[int(split_index):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ey1,Ey0 = np.sum(mus_T * p_T, axis = 1), np.sum(mus_C * p_C, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute gradient of loss function (n by 3* K - 1)\n",
    "dl1dtheta0 = pv_given_uvw - exposure_indicator_array[int(split_index):,:]\n",
    "dl1dtheta0 = dl1dtheta0[:, 1:] \n",
    "\n",
    "dl1dtheta1 = treatment_indicator_array[int(split_index):,:] * (pv_given_uvw - exposure_indicator_array[int(split_index):,:])\n",
    "dl2dmu = exposure_indicator_array[int(split_index):,:] * (mus_T -rewards_array)\n",
    "gradient_vector_l = np.concatenate([dl1dtheta0,dl1dtheta1,dl2dmu], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute gradient of H function (n by 3 * K - 1)\n",
    "dHdtheta0 = p_T * (mus_T - Ey1.reshape(mus_T.shape[0],1)) - p_C * (mus_C - Ey0.reshape(mus_C.shape[0],1))\n",
    "dHdtheta0 = dHdtheta0[:, 1:]\n",
    "dHdthetal = p_T * (mus_T - Ey1.reshape(mus_T.shape[0],1))\n",
    "dHdmu = p_T - p_C\n",
    "gradient_vector_H = np.concatenate([dHdtheta0,dHdthetal,dHdmu], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Hessian elements \n",
    "# k-1 by k-1 \n",
    "d2l2dtheta0 = - np.array([np.outer(row_, row_) for row_ in montecarlo_expected_probability_test])\n",
    "# k by k\n",
    "d2l2dtheta1 = -treatP * np.array([np.outer(row_, row_) for row_ in montecarlo_expected_probability_test])\n",
    "\n",
    "d2l2dtheta01 = -treatP * treatP * np.array([np.outer(row_, row_) for row_ in montecarlo_expected_probability_test])\n",
    "\n",
    "d2l2dmu = np.zeros(d2l2dtheta1.shape)\n",
    "## Change the diagonal element\n",
    "for i in range(d2l2dmu.shape[0]):\n",
    "    p_1minusp_i = (1 - montecarlo_expected_probability_test[i,:]) * (1 - montecarlo_expected_probability_test[i,:])\n",
    "    treatment_i = treatment_indicator_array[i,:]\n",
    "    exposure_i = exposure_indicator_array[i,:]\n",
    "    np.fill_diagonal(d2l2dtheta0[i], p_1minusp_i)\n",
    "    np.fill_diagonal(d2l2dtheta1[i], treatP* p_1minusp_i)\n",
    "    np.fill_diagonal(d2l2dmu[i], montecarlo_expected_probability_test[i,:])\n",
    "    np.fill_diagonal(d2l2dtheta01[i], treatP * p_1minusp_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l2dtheta0 = d2l2dtheta0[:,1:, 1:]\n",
    "d2l2dtheta01_k_m_1_by_k = d2l2dtheta01[:, 1:,:]\n",
    "d2l2dtheta10_k_by_k_m_1 = d2l2dtheta01[:, :,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concate\n",
    "Hessian_first_row = np.concatenate((d2l2dtheta0, d2l2dtheta01_k_m_1_by_k, np.zeros(d2l2dtheta01_k_m_1_by_k.shape)), axis =2)\n",
    "Hessian_second_row = np.concatenate((d2l2dtheta10_k_by_k_m_1, d2l2dtheta1, np.zeros(d2l2dtheta1.shape)), axis =2)\n",
    "Hessian_third_row = np.concatenate((np.zeros(d2l2dtheta10_k_by_k_m_1.shape), np.zeros(d2l2dmu.shape), d2l2dmu), axis =2)\n",
    "\n",
    "Hessian = np.concatenate((Hessian_first_row, Hessian_second_row, Hessian_third_row), axis = 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_finite = 0\n",
    "score_funcs = np.zeros(len(Hessian))\n",
    "for i in range(len(Hessian)):\n",
    "    if np.isfinite(np.linalg.cond(Hessian[i])):\n",
    "        score_funcs[i] = gradient_vector_H[i]@Hessian[i]@gradient_vector_l[i]\n",
    "        count_finite += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/250 [..............................] - ETA: 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 366us/step\n",
      "250/250 [==============================] - 0s 395us/step\n"
     ]
    }
   ],
   "source": [
    "res_tempt = np.array(myModelMultiple.predict(inputs_all_treat_3d_test)) - np.array(myModelMultiple.predict(inputs_all_control_3d_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "H_funcs_new = np.sum(res_tempt[:, :k] * predicted_outcomes_input[int(split_index):,:], axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_dim_estimate(vector_T, vector_C):\n",
    "    return np.mean(vector_T, vector_C), np.var(vector_T)/len(vecotr_T) + np.var(vector_C) / len(vector_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias_estimator(Hfuncs, influence_funcs):\n",
    "    return np.mean(Hfuncs - influence_funcs), np.var(influence_funcs) / len(influence_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment to Show the Performance of Estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m## Create expt environment\u001b[39;00m\n\u001b[1;32m      7\u001b[0m expt_717 \u001b[39m=\u001b[39m Environment_Smart_Recommendation_tiquan(user_pool, quota_pool, np\u001b[39m.\u001b[39marray([convert_string_to_vector(elm) \u001b[39mfor\u001b[39;00m elm \u001b[39min\u001b[39;00m finder_emb_pd\u001b[39m.\u001b[39miloc[finder_sample_ind,\u001b[39m1\u001b[39m]]))\n\u001b[0;32m----> 8\u001b[0m res_717_mixed, res_717_T, res_717_C, rew_seller, rew_seller_T, rew_seller_C \u001b[39m=\u001b[39m  expt_717\u001b[39m.\u001b[39;49mrun_experiment_parallel_world(\u001b[39m10000\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m T_obs \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39msum(elm[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m elm \u001b[39min\u001b[39;00m res_717_T]\n\u001b[1;32m     11\u001b[0m C_obs \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39msum(elm[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m elm \u001b[39min\u001b[39;00m res_717_C]\n",
      "Cell \u001b[0;32mIn[64], line 143\u001b[0m, in \u001b[0;36mEnvironment_Smart_Recommendation_tiquan.run_experiment_parallel_world\u001b[0;34m(self, T, user_arrival_probability, verbose)\u001b[0m\n\u001b[1;32m    140\u001b[0m user \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muserPool[user_coming_index]\n\u001b[1;32m    141\u001b[0m \u001b[39m## Step 2: recommend some videos to the user; since we know the ground truth - the recommend_in_parallel would also return the \u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m## quantities as if all the sellers were treated or controlled  \u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m candidates_, probs,similarity_scores, expose_vector, treat_indicator, good_indicator, u_feat, vu_feats, e_T, e_C, p_T, p_C \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecommend_in_parallel(user)\n\u001b[1;32m    145\u001b[0m \u001b[39m## Step 3: Generate outcomes(rewards), and record the status \u001b[39;00m\n\u001b[1;32m    146\u001b[0m rewards \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[64], line 75\u001b[0m, in \u001b[0;36mEnvironment_Smart_Recommendation_tiquan.recommend_in_parallel\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     73\u001b[0m candidates_, similarity_scores_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(query_result[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]), np\u001b[39m.\u001b[39marray(query_result[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m     74\u001b[0m \u001b[39m## Heterogeneity: adding a constant to all the good sellers \u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m good_indicator \u001b[39m=\u001b[39m  np\u001b[39m.\u001b[39marray([ind_i \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoodQuotaIDs \u001b[39mfor\u001b[39;49;00m ind_i \u001b[39min\u001b[39;49;00m candidates_])\n\u001b[1;32m     76\u001b[0m treat_indicator \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([ind_i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtreatQuotaIDs \u001b[39mfor\u001b[39;00m ind_i \u001b[39min\u001b[39;00m candidates_])\n\u001b[1;32m     78\u001b[0m added_similarity \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m treat_indicator \u001b[39m*\u001b[39m good_indicator\n",
      "Cell \u001b[0;32mIn[64], line 75\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m candidates_, similarity_scores_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(query_result[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]), np\u001b[39m.\u001b[39marray(query_result[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m     74\u001b[0m \u001b[39m## Heterogeneity: adding a constant to all the good sellers \u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m good_indicator \u001b[39m=\u001b[39m  np\u001b[39m.\u001b[39marray([ind_i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoodQuotaIDs \u001b[39mfor\u001b[39;00m ind_i \u001b[39min\u001b[39;00m candidates_])\n\u001b[1;32m     76\u001b[0m treat_indicator \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([ind_i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtreatQuotaIDs \u001b[39mfor\u001b[39;00m ind_i \u001b[39min\u001b[39;00m candidates_])\n\u001b[1;32m     78\u001b[0m added_similarity \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m treat_indicator \u001b[39m*\u001b[39m good_indicator\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "B = 100 \n",
    "truth_list = []\n",
    "for b in range(B):\n",
    "    if b % 10 ==0:\n",
    "        print(b)\n",
    "    ## Create expt environment\n",
    "    expt_717 = Environment_Smart_Recommendation_tiquan(user_pool, quota_pool, np.array([convert_string_to_vector(elm) for elm in finder_emb_pd.iloc[finder_sample_ind,1]]))\n",
    "    res_717_mixed, res_717_T, res_717_C, rew_seller, rew_seller_T, rew_seller_C =  expt_717.run_experiment_parallel_world(10000)\n",
    "\n",
    "    T_obs = [np.sum(elm[1]) for elm in res_717_T]\n",
    "    C_obs = [np.sum(elm[1]) for elm in res_717_C]\n",
    "    truth_list += [np.mean(T_obs) - np.mean(C_obs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 0., 1., 5., 2., 3., 1., 1., 1.]),\n",
       " array([10.4195245 , 10.44381024, 10.46809598, 10.49238172, 10.51666746,\n",
       "        10.54095321, 10.56523895, 10.58952469, 10.61381043, 10.63809617,\n",
       "        10.66238191]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
