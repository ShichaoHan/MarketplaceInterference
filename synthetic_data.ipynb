{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Model \n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_ranking as tfr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_probability_exposure_examination(probs_matrix, exposure_matrix): \n",
    "    aggregate_probs = np.mean(probs_matrix, axis = 0)\n",
    "    exposure_probs = np.mean(exposure_matrix, axis = 0)\n",
    "    ## Euclidean distance \n",
    "    euc_dist = np.linalg.norm(aggregate_probs - exposure_probs)\n",
    "    ## NDCG LOSS \n",
    "    y_true = tf.ragged.constant(exposure_matrix)\n",
    "    y_pred = tf.ragged.constant(probs_matrix)\n",
    "    loss_NDCG = tfr.keras.losses.ApproxNDCGLoss(ragged=True)\n",
    "    NDCG_loss_result = loss_NDCG(y_true, y_pred).numpy()\n",
    "    return euc_dist, aggregate_probs, exposure_probs, NDCG_loss_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "x = np.linspace(-4, 4, 100)\n",
    "tencent_blue = (0,0.3215686274509804,0.8509803921568627)\n",
    "tencent_orange = (0.9333333333333333, 0.49411764705882355, 0.2784313725490196)\n",
    "\n",
    "\n",
    "# Calculate y-values for the standard normal density curve\n",
    "y_standard_normal = (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mySoftMax(arr):\n",
    "    num = np.exp(arr)\n",
    "    denom = np.sum(num)\n",
    "    return num/denom\n",
    "\n",
    "\n",
    "def naive_est(res):\n",
    "    treat_res = [elm[0] for elm in res[0]]\n",
    "    control_res = [elm[1] for elm in res[0]]\n",
    "    return np.mean(treat_res) - np.mean(control_res)\n",
    "\n",
    "\n",
    "def dim_est(obs_T, obs_C):\n",
    "    n1,n0 = len(obs_T), len(obs_C)\n",
    "    return np.mean(obs_T) -np.mean(obs_C), np.sqrt(np.var(obs_T)/n1 + np.var(obs_C) / n0)\n",
    "\n",
    "\n",
    "def point_est(all_treat_array, all_control_array):\n",
    "    mus_T, mus_C  = all_treat_array[:, 11:21], all_control_array[:,11:21]\n",
    "    p_T, p_C  = all_treat_array[:, 21:], all_control_array[:,21:]\n",
    "    return np.mean(np.sum((mus_T * (p_T - p_C)), axis = 1 ))\n",
    "\n",
    "\n",
    "def naive_dim_estimate(vector_T, vector_C):\n",
    "    return np.mean(vector_T) - np.mean(vector_C), np.var(vector_T)/len(vector_T) + np.var(vector_C) / len(vector_C)\n",
    "\n",
    "def debias_estimator(Hfuncs, debias_terms):\n",
    "    score_functions = Hfuncs - debias_terms \n",
    "    undebiased_estimator = np.mean(Hfuncs)\n",
    "    debiased_estimator = np.mean(score_functions)\n",
    "    variance_estimator = np.mean((score_functions - debiased_estimator)**2) /len(score_functions)\n",
    "    return debiased_estimator, variance_estimator, undebiased_estimator \n",
    "\n",
    "def is_invertible(matrix):\n",
    "    return np.linalg.det(matrix) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## True Model \n",
    "\n",
    "class MyModel_True:\n",
    "    def __init__(self, k, num_treats,promo_ratio):\n",
    "\n",
    "        self.k = k\n",
    "        self.promo_ratio = promo_ratio\n",
    "        self.num_treats = num_treats\n",
    "\n",
    "    def predict(self,inputs):\n",
    "        Q_input = inputs.shape[0]\n",
    "        split_structure =  [1]+ [1] + [1] * self.num_treats + [1]\n",
    "        splitted_elements = tf.split(inputs, split_structure, axis=2)\n",
    "        X_utility =  np.squeeze(np.array(splitted_elements[1]), axis=2)\n",
    "        X_goodbads = np.squeeze(np.array(splitted_elements[0]), axis = 2)\n",
    "\n",
    "        W_matrix =  np.squeeze(np.array(splitted_elements[2]), axis =2 )\n",
    "\n",
    "        final_score_matrix = (1 + W_matrix * self.promo_ratio * X_goodbads) * X_utility\n",
    "\n",
    "        ## First element of each row \n",
    "        first_elm = X_utility[:,0]\n",
    "        minus_matrix = first_elm.reshape((len(first_elm),1))@np.ones((1,K))\n",
    "        final_score_matrix_normalized = final_score_matrix - minus_matrix\n",
    "        ## Correct exposure probability \n",
    "        X_logit = np.apply_along_axis(logistic_row, axis=1, arr=final_score_matrix_normalized)\n",
    "    \n",
    "        expose_indices = np.argmax(X_logit, axis = 1)\n",
    "        inddds = np.array(list(np.arange(K)) * Q_input).reshape(Q_input,K)\n",
    "        exposure_matrix = np.array([inddds[i,:] == expose_indices[i] for i in range(Q_input)])\n",
    "\n",
    "        ## Outcome model  \n",
    "        \n",
    "        ## First: a true outcome model of Exponential \n",
    "        outcome_potential = np.random.normal(size=(Q_input, K)) +  X_utility\n",
    "        pred_out = np.sum(exposure_matrix * outcome_potential, axis = 1 )\n",
    "        pred_out = pred_out.reshape(pred_out.shape[0], 1 )\n",
    "        return np.concatenate([X_logit, pred_out], axis = 1 )\n",
    "## True Model \n",
    "\n",
    "class MyModel_Random:\n",
    "    def __init__(self, k, num_treats,promo_ratio):\n",
    "\n",
    "        self.k = k\n",
    "        self.promo_ratio = promo_ratio\n",
    "        self.num_treats = num_treats\n",
    "\n",
    "    def predict(self,inputs):\n",
    "        output_shape = np.array(input_3d_test_treat.shape)[:2]\n",
    "        array = np.random.rand(output_shape[0],output_shape[1])\n",
    "        # Compute the sum of each row\n",
    "        row_sums = np.sum(array, axis=1)\n",
    "\n",
    "        # Reshape the row sums to make them compatible for broadcasting\n",
    "        row_sums = row_sums.reshape(-1, 1)\n",
    "\n",
    "        normalized_array = array / row_sums\n",
    "\n",
    "        return normalized_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of videos \n",
    "J = 50 \n",
    "## Consideration set size \n",
    "K = 10 \n",
    "k=10\n",
    "## Generate some queries along with the recommendation model \n",
    "Q = 1000\n",
    "\n",
    "\n",
    "def permute_treatment_dict(J):\n",
    "    perm_dict = {}\n",
    "    for j in range(J):\n",
    "        perm_dict[j] = np.random.choice([True,False], 1)\n",
    "    return perm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## True Exposure Model and Data Generating Process \n",
    "def logistic_row(row):\n",
    "    return np.exp(row) / np.sum(np.exp(row))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# I. deterministic: \n",
    "# selected_indices = np.argmax(X_logit, axis = 1)\n",
    "# II. random choice for exposure \n",
    "\n",
    "def DGP(promo_rat, K,Q, J):\n",
    "    ## Generate a utility score for each viewer and video pair\n",
    "    utility_score_matrix = np.exp(np.random.normal(size=(Q,J)))\n",
    "    good_bad_dict = {} \n",
    "    treatment_dict = {} \n",
    "    utility_score = {} \n",
    "    for j in range(J):\n",
    "        good_bad_dict[j] = np.random.choice([True,False], 1)\n",
    "        treatment_dict[j] = np.random.choice([True,False], 1)\n",
    "        utility_score[j] = np.random.uniform()\n",
    "    X_goodbads = []\n",
    "    X_utility = []\n",
    "    W_matrix = []\n",
    "    query_matrix = []\n",
    "    promo_ratio = promo_rat\n",
    "    for each_query in range(Q):\n",
    "        ## Form the consideration set \n",
    "        selected_indices = np.random.choice(np.arange(J), K, replace= False)\n",
    "        query_matrix += [selected_indices]\n",
    "        X_goodbads = np.append(X_goodbads,[good_bad_dict[ind] for ind in selected_indices])\n",
    "        X_utility = np.append(X_utility, [utility_score_matrix[each_query, ind] for ind in selected_indices])\n",
    "        W_matrix = np.append(W_matrix, [treatment_dict[ind] for ind in selected_indices])\n",
    "    X_goodbads = X_goodbads.reshape(Q, K)\n",
    "    X_utility = X_utility.reshape(Q, K)\n",
    "    W_matrix = W_matrix.reshape(Q,K)\n",
    "    final_score_matrix = (1 + W_matrix * promo_ratio * X_goodbads) * X_utility\n",
    "\n",
    "    X_logit = np.apply_along_axis(logistic_row, axis=1, arr=final_score_matrix)\n",
    "    expose_indices = np.array([np.random.choice(np.arange(K), size = 1, p = X_logit[i,:]) for i in range(Q)])\n",
    "    inddds = np.array(list(np.arange(K)) * Q).reshape(Q,K)\n",
    "    exposure_matrix = np.array([inddds[i,:] == expose_indices[i] for i in range(Q)])\n",
    "\n",
    "    ## Outcome model  \n",
    "    ## First: a true outcome model of Exponential \n",
    "    outcome_potential = np.random.normal(size=(Q, K)) +  X_utility\n",
    "\n",
    "    return query_matrix,X_goodbads,X_utility,W_matrix, exposure_matrix, outcome_potential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_B, dim_var_B= [],[]\n",
    "debias_B_true, debias_var_B_true = [],[] \n",
    "undebias_B_true, debias_var_old_B_true = [],[]\n",
    "truth= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start K = 20, Q = 1000, J = 50\n",
      "0\n",
      "Start K = 5, Q = 1000, J = 100\n",
      "0\n",
      "Start K = 10, Q = 1000, J = 100\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## Number of iterations of DGP\n",
    "B = 100\n",
    "## Number of videos \n",
    "JQ_sizes = [(50,1000), (100,1000), (200, 1000)]\n",
    "training_ratio = 0.4\n",
    "\n",
    "## Consideration set size \n",
    "Ks = [5,10,20]\n",
    "\n",
    "\n",
    "for (J, Q) in JQ_sizes:\n",
    "    for K in Ks:\n",
    "        if J == 50 and Q == 1000 and K < 20:\n",
    "            continue \n",
    "        print(\"Start K = {}, Q = {}, J = {}\".format(str(K), str(Q), str(J)))\n",
    "        dim_B, dim_var_B= [],[]\n",
    "        debias_B_true, debias_var_B_true = [],[] \n",
    "        undebias_B_true, debias_var_old_B_true = [],[]\n",
    "        truth= []\n",
    "\n",
    "\n",
    "        ## True Outcome Model test \n",
    "        L = 1\n",
    "        ith_treat = 0\n",
    "        M = 500 \n",
    "        groupNames = [0,1]\n",
    "        uplift_ratio = 0\n",
    "        k = K\n",
    "        for b in range(B):\n",
    "            if b % 20 == 0:\n",
    "                print(b)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            ## DGP and data pre-processing \n",
    "            query_matrix,X_goodbads,X_utility,W_matrix, exposure_matrix, outcome_potential = DGP(uplift_ratio, K,Q,J)\n",
    "            query_train, query_test = np.array(query_matrix)[:int(training_ratio * Q),:], np.array(query_matrix)[int(training_ratio * Q):,:]\n",
    "            X_goodbads_train, X_goodbads_test = X_goodbads[:int(training_ratio * Q),:], X_goodbads[int(training_ratio * Q):,:]\n",
    "            X_utility_train, X_utility_test =  X_utility[:int(training_ratio* Q),:], X_utility[int(training_ratio * Q):,:]\n",
    "            W_matrix_train, W_matrix_test = W_matrix[:int(training_ratio * Q),:], W_matrix[int(training_ratio * Q):,:]\n",
    "            observed_queries_treatment = np.sum(exposure_matrix * W_matrix, axis = 1 )\n",
    "            observed_outcome = np.sum(outcome_potential * exposure_matrix, axis = 1 )\n",
    "\n",
    "            T, C = observed_outcome[observed_queries_treatment == groupNames[ith_treat + 1 ]] , observed_outcome[observed_queries_treatment == 0]  \n",
    "            exposure_matrix_train,exposure_matrix_test = exposure_matrix[:int(training_ratio * Q),:], exposure_matrix[int(training_ratio * Q):,:]\n",
    "            outcome_matrix = exposure_matrix * outcome_potential\n",
    "            outcome_matrix = np.sum(outcome_matrix, axis = 1 ).reshape(outcome_matrix.shape[0],1)\n",
    "\n",
    "            observed_outcome_train, observed_outcome_test = observed_outcome[:int(training_ratio * Q)], observed_outcome[int(training_ratio * Q):]\n",
    "            outcome_matrix_train, outcome_matrix_test = outcome_matrix[:int(training_ratio * Q)], outcome_matrix[int(training_ratio * Q):]\n",
    "            outcome_potential_train, outcome_potential_test = outcome_potential[:int(training_ratio * Q)],outcome_potential[int(training_ratio * Q):] \n",
    "            inputs_3d_train = tf.stack([X_goodbads_train,X_utility_train, W_matrix_train, X_utility_train ], axis = -1)\n",
    "            inputs_3d_test = tf.stack([X_goodbads_test,X_utility_test, W_matrix_test, X_utility_test], axis = -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            input_3d_test_treat = tf.stack([X_goodbads_test,X_utility_test, np.ones(W_matrix_test.shape), X_utility_test], axis = -1)\n",
    "            input_3d_test_control = tf.stack([X_goodbads_test,X_utility_test, np.zeros(W_matrix_test.shape), X_utility_test], axis = -1)\n",
    "            output_3d_train = tf.concat([tf.cast(exposure_matrix_train, dtype=float),outcome_matrix_train], axis = 1)\n",
    "            output_3d_test = tf.concat([tf.cast(exposure_matrix_test, dtype=float),outcome_matrix_test ], axis = 1)\n",
    "\n",
    "            exposure_indicator_array = exposure_matrix_test\n",
    "\n",
    "\n",
    "            ## Get treatment indicator matrix\n",
    "            w_dict = {}\n",
    "\n",
    "            for l in range(L):\n",
    "                w_dict[l] = tf.convert_to_tensor(W_matrix == groupNames[l+1], dtype = float)\n",
    "\n",
    "            training_num = int(W_matrix.shape[0] * training_ratio)\n",
    "            testing_num = W_matrix.shape[0] - training_num\n",
    "\n",
    "\n",
    "            w_all_treat = tf.convert_to_tensor(np.array([[1] * k for _ in range(W_matrix.shape[0])],dtype='float32'))\n",
    "            w_all_control = tf.convert_to_tensor(np.array([[0] * k for _ in range(W_matrix.shape[0])],dtype='float32'))\n",
    "\n",
    "            inputs_all_treat_3d = tf.stack([X_goodbads,X_utility] + [w_all_treat if l == ith_treat else w_all_control for l in range(L)] +[ X_utility], axis = 2)\n",
    "            inputs_all_control_3d = tf.stack([X_goodbads,X_utility] + [w_all_control if l == ith_treat else w_all_control for l in range(L)] +[ X_utility ], axis = 2)\n",
    "            inputs_all_treat_3d = tf.cast(inputs_all_treat_3d, dtype = 'float32')\n",
    "            inputs_all_control_3d = tf.cast(inputs_all_control_3d, dtype = 'float32')\n",
    "\n",
    "            ## All other all_treated \n",
    "            inputs_all_treat_3d_dict = {} \n",
    "            for l in range(L):\n",
    "                inputs_all_treat_3d_l = tf.stack([X_goodbads,X_utility] + [w_all_treat if l == v else w_all_control for v in range(L)] +[ X_utility ], axis = 2)\n",
    "                #inputs_all_treat_3d_l = tf.stack([x_basebid, x_sort_score, x_bid,x_ecpm, x_cvr] + [w_all_treat if v == l else w_all_control for v in range(L)] + [x_cvr], axis = 2)\n",
    "                inputs_all_treat_3d_dict[l] = tf.cast(inputs_all_treat_3d_l, dtype = 'float32')\n",
    "\n",
    "            exposure_indicator_outcome_train, exposure_indicator_outcome_test = outcome_matrix_train, outcome_matrix_test\n",
    "            inputs_all_treat_3d_test = input_3d_test_treat\n",
    "            inputs_all_control_3d_test = input_3d_test_control\n",
    "            is_selected_indicator_train,is_selected_indicator_test = exposure_matrix_train,exposure_matrix_test\n",
    "\n",
    "            treat_control_dict = {} \n",
    "            for l in range(L):\n",
    "                inputs_3d_train_l = inputs_all_treat_3d_dict[l][:training_num,:,:]\n",
    "                inputs_3d_test_l = inputs_all_treat_3d_dict[l][training_num:,:,:]\n",
    "                treat_control_dict[l] = {'train':inputs_3d_train_l, 'test': inputs_3d_test_l}\n",
    "\n",
    "            myModelMultiple = MyModel_True(K, L, uplift_ratio)\n",
    "            # myModelMultiple.compile(loss=custom_loss,optimizer=tf.keras.optimizers.legacy.Adam())\n",
    "            # myModelMultiple.fit(input_3d_train,output_3d_train , epochs=10, verbose=False)\n",
    "            exposure_indicator_array = is_selected_indicator_test\n",
    "            treatment_indicator_array = 1 * (np.array(w_dict[ith_treat])[training_num:,:])\n",
    "\n",
    "            res_tempt = np.array(myModelMultiple.predict(inputs_all_treat_3d_test)) - np.array(myModelMultiple.predict(inputs_all_control_3d_test))\n",
    "\n",
    "\n",
    "            pred_H_new = np.array(myModelMultiple.predict(inputs_all_treat_3d_test)) - np.array(myModelMultiple.predict(inputs_all_control_3d_test))\n",
    "            model_pred_H = np.array(myModelMultiple.predict(inputs_3d_test))\n",
    "            model_pred_all_treat = myModelMultiple.predict(inputs_all_treat_3d_test)\n",
    "            model_pred_all_control = myModelMultiple.predict(inputs_all_control_3d_test)\n",
    "            all_treat_array, all_control_array = np.array(model_pred_all_treat), np.array(model_pred_all_control)\n",
    "\n",
    "            ## All other counterfactuals \n",
    "            counterfactual_pred_dict = {} \n",
    "            for l in range(L):\n",
    "                model_pred_all_l = myModelMultiple.predict(treat_control_dict[l]['test'])\n",
    "                counterfactual_pred_dict[l] = model_pred_all_l\n",
    "\n",
    "            mus_T, mus_C  = outcome_potential_test,outcome_potential_test\n",
    "            p_T, p_C  = all_treat_array[:, :k], all_control_array[:,:k]\n",
    "            rewards_array = observed_outcome_test\n",
    "            rewards_array = rewards_array.reshape(rewards_array.shape[0],1)\n",
    "            Ey1,Ey0 = np.sum(mus_T * p_T, axis = 1), np.sum(mus_C * p_C, axis = 1)\n",
    "            pv1,pv0 = np.sum(exposure_indicator_array * p_T, axis = 1), np.sum(exposure_indicator_array * p_C, axis = 1)\n",
    "\n",
    "            pv_given_uvw = p_T * treatment_indicator_array + p_C * (1 - treatment_indicator_array)\n",
    "\n",
    "\n",
    "            p_realized = model_pred_H[:,:K]\n",
    "\n",
    "\n",
    "\n",
    "            ## 1. COMPUTE THE GRADIENT OF LOSSS  \n",
    "            ## FIX: change to realized outcome \n",
    "            #dl1dtheta0 = pv_given_uvw - exposure_indicator_array\n",
    "            dl1dtheta0 = p_realized - exposure_indicator_array\n",
    "            dl1dtheta0 = dl1dtheta0[:, 1:] \n",
    "\n",
    "\n",
    "            ## FIX: iterate over all L \n",
    "            dl1dthetal_dict = {} \n",
    "            for l in range(L):\n",
    "                treatment_indicator_array_l = w_dict[l][training_num:, :]\n",
    "                dl1dthetal_dict[l] = treatment_indicator_array_l *  (p_realized - exposure_indicator_array)\n",
    "            dl2dmu = exposure_indicator_array * (mus_T -rewards_array)\n",
    "            gradient_vector_l = np.concatenate([dl1dtheta0]+[dl1dthetal_dict[l] for l in range(L)] +[dl2dmu], axis =1 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ## 2. COMPUTE  THE GRADIENT OF H FUNCTION\n",
    "            dHdtheta0 = p_T * (mus_T - Ey1.reshape(mus_T.shape[0],1)) - p_C * (mus_C - Ey0.reshape(mus_C.shape[0],1))\n",
    "            dHdtheta0 = dHdtheta0[:, 1:]\n",
    "\n",
    "\n",
    "\n",
    "            ## FIX: iterate over each l \n",
    "            dHdthetal_dict = {} \n",
    "            for l in range(L):\n",
    "\n",
    "                p_T_thetal = counterfactual_pred_dict[l][:,:k]\n",
    "                Eyl = np.sum(mus_T * p_T_thetal, axis = 1)\n",
    "                dHdthetal_dict[l] = p_T_thetal * (mus_T - Eyl.reshape(mus_T.shape[0],1))\n",
    "                ## 0 for the groups that are not the target treatment group \n",
    "                if l != ith_treat:\n",
    "                    dHdthetal_dict[l] = 0 * (p_T_thetal * (mus_T - Eyl.reshape(mus_T.shape[0],1)))\n",
    "\n",
    "            #dHdthetal = p_T * (mus_T - Ey1.reshape(mus_T.shape[0],1))\n",
    "            dHdmu = p_T - p_C\n",
    "            #gradient_vector_H = np.concatenate([dHdtheta0,dHdthetal,dHdmu], axis =1 )\n",
    "\n",
    "            ## FIX: iterate over all l \n",
    "            gradient_vector_H = np.concatenate([dHdtheta0]+[dHdthetal_dict[l] for l in range(L)]+[dHdmu], axis =1 )\n",
    "            ## Gradient over all other treatments \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ## 3. FIND THE EXPECTATION OF HESSIAN MATRIX \n",
    "\n",
    "\n",
    "\n",
    "            Hessian_all = np.zeros((inputs_3d_test.shape[0],(L+2) * K - 1,  (L+2) * K - 1))\n",
    "\n",
    "            montecarlo_expected_probability = np.zeros(exposure_indicator_array.shape)\n",
    "\n",
    "            selected_indicator_dict  = {}\n",
    "            assignment_pd_dict = {} \n",
    "            dmu_dict = {} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for m in range(M):\n",
    "                treat_dict_m = permute_treatment_dict(J)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            M = 500\n",
    "            for m in range(M):\n",
    "                w_dict_m = {} \n",
    "                treat_dict_m = permute_treatment_dict(J)\n",
    "                W_matrix_m = []\n",
    "                for i in range(np.array(query_matrix).shape[0]):\n",
    "                    ## Form the consideration set \n",
    "                    each_query=np.array(query_matrix)[i,:]\n",
    "                    W_matrix_m = np.append(W_matrix_m, [[treat_dict_m[ind] for ind in each_query]])\n",
    "\n",
    "                W_matrix_m = W_matrix_m.reshape(np.array(query_matrix).shape)\n",
    "\n",
    "\n",
    "\n",
    "                for l in range(L):\n",
    "                    w_dict_m[l] = tf.convert_to_tensor(W_matrix_m == groupNames[l + 1], dtype = float)\n",
    "\n",
    "\n",
    "                inputs_3d_m = tf.stack([X_goodbads,X_utility]+  [w_dict_m[l] for l in range(L)] +[X_goodbads], axis = -1)\n",
    "                inputs_3d_test_m = inputs_3d_m[training_num:,:]\n",
    "                model_pred_m = np.array(myModelMultiple.predict(inputs_3d_test_m))[:,:k]\n",
    "                outer_product_pv1pv2 = np.array([np.outer(row_[1:], row_[1:]) for row_ in model_pred_m])\n",
    "                outer_product_treatment_indicator = np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                outer_product_pv1_one_minus_pv2 = np.array([np.outer(row_, 1-row_) for row_ in model_pred_m])\n",
    "                p_treat = 1/(L+1) \n",
    "\n",
    "                is_selected_indicator_train,is_selected_indicator_test =np.array(exposure_matrix[:training_num,:]), np.array(exposure_matrix[training_num:,:])\n",
    "                selected_indicator_dict[m] = is_selected_indicator_test \n",
    "                d2l2dtheta0 = - np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "\n",
    "                ## FIX: Iterate over l \n",
    "                d2l2dthetal_dict = {}\n",
    "                for l in range(L):\n",
    "                    ## K by K \n",
    "\n",
    "                    w_m_l = np.array(w_dict_m[l][training_num:,:])\n",
    "\n",
    "                    ## Off-diagonal terms \n",
    "                    # d2l2dtheta1 =  np.array([np.outer(row_, row_) for row_ in w_m_l]) * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                    d2l2dtheta1 = - p_treat * p_treat * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                    ## Modify diagonal terms\n",
    "                    for i in range(d2l2dtheta1.shape[0]):\n",
    "                        treat_indicator_i = w_m_l[i,:]\n",
    "                        probs_i = model_pred_m[i,:]\n",
    "                        # np.fill_diagonal(d2l2dtheta1[i],treat_indicator_i * probs_i * (1-probs_i))\n",
    "                        np.fill_diagonal(d2l2dtheta1[i], p_treat * probs_i * (1-probs_i))\n",
    "                    d2l2dthetal_dict[l] = d2l2dtheta1\n",
    "\n",
    "\n",
    "\n",
    "                ## FIX: iterate over all l1, l2 \n",
    "                d2ldthetal1dthetal2 = {} \n",
    "                for l in range(L):\n",
    "                    w_m_l = np.array(w_dict_m[l][training_num:,:])\n",
    "                    ## Off-diagonal terms \n",
    "                    #d2l2dtheta0dtheta1 = - np.multiply(w_m_l[:,np.newaxis], np.array([np.outer(row_, row_) for row_ in model_pred_m]))\n",
    "                    d2l2dtheta0dtheta1 = - p_treat * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                    for i in range(d2l2dtheta0dtheta1.shape[0]):\n",
    "                        treat_indicator_i = w_m_l[i,:]\n",
    "                        p_1minusp_i = model_pred_m[i,:] * (1 - model_pred_m[i,:])\n",
    "                        #np.fill_diagonal(d2l2dtheta0dtheta1[i], treat_indicator_i * p_1minusp_i)\n",
    "                        np.fill_diagonal(d2l2dtheta0dtheta1[i], p_treat * p_1minusp_i)\n",
    "\n",
    "                    ## NOTE: -1 to indicate the baseline theta \n",
    "                    d2ldthetal1dthetal2[(-1,l)] = d2l2dtheta0dtheta1[:,1:,:]\n",
    "                    d2ldthetal1dthetal2[(l,-1)] = np.transpose(d2l2dtheta0dtheta1[:,1:,:], (0,2,1))\n",
    "                    for l_prime in range(L):\n",
    "                        if l != l_prime: \n",
    "                            #w_m_l = np.array(w_dict_m[l][training_num:,:])\n",
    "                            #w_m_l_prime = np.array(w_dict_m[l_prime][training_num:,:])\n",
    "                            #indicator_outer = np.array([np.outer(w_m_l[i,:], w_m_l_prime[i,:]) for i in range(w_m_l.shape[0])])\n",
    "\n",
    "                            #d2l2dthetal1dthetal2 = -  indicator_outer * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                            d2l2dthetal1dthetal2 = -  p_treat * p_treat * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                            d2ldthetal1dthetal2[(l,l_prime)]  = d2l2dthetal1dthetal2\n",
    "                            d2ldthetal1dthetal2[(l_prime,l)]  = np.transpose(d2l2dthetal1dthetal2, (0,2,1))\n",
    "                        else:\n",
    "                            d2ldthetal1dthetal2[(l,l)] = d2l2dthetal_dict[l]\n",
    "\n",
    "\n",
    "                d2l2dmu = np.zeros(d2l2dtheta1.shape)\n",
    "\n",
    "                # treatment_indicator_array_m = \n",
    "                for i in range(d2l2dmu.shape[0]):\n",
    "                    #p_1minusp_i = (1 - model_pred_m[i,:]) * (1 - model_pred_m[i,:])\n",
    "                    p_1minusp_i = model_pred_m[i,:] * (1 - model_pred_m[i,:])\n",
    "                    # treatment_i = treatment_indicator_array[i,:]\n",
    "                    # exposure_i = is_selected_indicator_test[i,:]\n",
    "                    np.fill_diagonal(d2l2dtheta0[i], p_1minusp_i)\n",
    "                    np.fill_diagonal(d2l2dmu[i], model_pred_m[i,:])\n",
    "\n",
    "\n",
    "\n",
    "                d2l2dtheta0 = d2l2dtheta0[:,1:, 1:]\n",
    "                # d2l2dtheta01_k_m_1_by_k = d2l2dtheta01[:, 1:,:]\n",
    "                # d2l2dtheta10_k_by_k_m_1 = d2l2dtheta01[:, :,1:]\n",
    "                Hessian_first_row = np.concatenate([d2l2dtheta0] + [d2ldthetal1dthetal2[(-1, l)] for l in range(L)] + [np.zeros((d2l2dtheta0.shape[0], K-1, K))], axis =2)\n",
    "\n",
    "                ## 1 to L + 1 row \n",
    "                Hessian_middle_dict = {}\n",
    "                for l in range(L):\n",
    "                    row_l = np.concatenate([d2ldthetal1dthetal2[(l, -1)]] + [d2ldthetal1dthetal2[(l, l_prime)] for l_prime in range(L)] +[np.zeros((d2l2dtheta0.shape[0], K, K))], axis =2)\n",
    "\n",
    "                    Hessian_middle_dict[l] = row_l                                                                           \n",
    "\n",
    "\n",
    "                Hessian_third_row = np.concatenate((np.zeros((d2l2dtheta0.shape[0], K, K  * (L + 1 ) - 1 )), d2l2dmu), axis =2)\n",
    "\n",
    "                Hessian = np.concatenate([Hessian_first_row] + [Hessian_middle_dict[l] for l in range(L)] + [Hessian_third_row], axis = 1 )\n",
    "\n",
    "                dmu_dict[m] = d2l2dmu\n",
    "\n",
    "                Hessian_all = Hessian_all + Hessian\n",
    "\n",
    "            Hessian_final = Hessian_all / M\n",
    "            count_finite = 0\n",
    "            score_funcs = np.zeros(len(Hessian_final))\n",
    "            for i in range(len(Hessian_final)):\n",
    "                if is_invertible(Hessian_final[i]):\n",
    "                    score_funcs[i] = gradient_vector_H[i]@np.linalg.inv(Hessian_final[i])@gradient_vector_l[i]\n",
    "                    count_finite += 1 \n",
    "            outs_1 = res_tempt[score_funcs !=0,K]\n",
    "            debias_point,  debias_var, undebiased_point  = debias_estimator(outs_1, score_funcs[score_funcs!=0])\n",
    "            dim_point, dim_var = dim_est(T, C)\n",
    "            dim_B += [dim_point]\n",
    "            dim_var_B += [dim_var]\n",
    "            debias_B_true += [debias_point]\n",
    "            debias_var_B_true += [debias_var]\n",
    "            undebias_B_true += [undebiased_point]\n",
    "\n",
    "        result_df = pd.DataFrame({\"debias_point\": debias_B_true, \"debias_var\":debias_var_B_true, \"dim\": dim_B, \n",
    "                                 \"dim_var\":dim_var_B, \"undebias_point\": undebias_B_true, \"J\" : J,\"Q\": Q, \"K\":K })\n",
    "        result_df.to_csv(\"result/synthetic_aa_j{}q{}k{}_100.csv\".format(str(J), str(Q), str(K)))\n",
    "        plt.figure() \n",
    "        sns.kdeplot(np.array(debias_B_true) / np.sqrt(np.array(debias_var_B_true)) , shade = True,color=tencent_blue,label = \"Ours\",alpha=0.1)\n",
    "        sns.kdeplot(np.array(dim_B) / np.sqrt(np.array(dim_var_B)), shade = True,color=tencent_orange,label = \"DIM\",alpha=0.1)\n",
    "        plt.plot(x, y_standard_normal, color='black', label=\"Standard Normal\", ls='--')\n",
    "        plt.legend()\n",
    "        plt.savefig(\"result/synthetic_aa_j{}q{}k{}_density.png\".format(str(J), str(Q), str(K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
