{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2499e22e-d3ba-40e0-bcc6-8f4916598af3",
   "metadata": {},
   "source": [
    "# 初始化spark环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733dbbd3-ab40-4d7d-ab9e-9505ffd4cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model \n",
    "from utils_empirical import * \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe3cb64-baa2-42cc-9588-1cad0a58611a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed550461-81ce-46fc-9a20-5f14e59bd97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062cb95-a8a3-443a-927a-8324f843bda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66cd1557-8987-4e4f-9564-8e629034280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow_ranking as tfr \n",
    "from scipy.special import kl_div\n",
    "import numpy as np \n",
    "\n",
    "def dim_est_naive(obs_T, obs_C):\n",
    "    n1,n0 = len(obs_T), len(obs_C)\n",
    "    return np.mean(obs_T) -np.mean(obs_C), np.sqrt(np.var(obs_T)/n1 + np.var(obs_C) / n0)\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow_ranking as tfr \n",
    "## Some Helper Functions \n",
    "def find_cosine(A_vec, B_vec):\n",
    "    A = np.array([float(i) for i in A_vec])\n",
    "    B = np.array([float(i) for i in B_vec])\n",
    "    return float(np.dot(A,B)/(np.linalg.norm(A,2)*np.linalg.norm(B,2)))\n",
    "\n",
    "def convert_string_to_vector(s):\n",
    "    res = s.split(\",\")\n",
    "    return np.array(res).astype(float)\n",
    "\n",
    "def find_cosine_string(s1, s2):\n",
    "    return find_cosine(convert_string_to_vector(s1), convert_string_to_vector(s2))\n",
    "\n",
    "def mySoftMax(arr):\n",
    "    num = np.exp(arr)\n",
    "    denom = np.sum(num)\n",
    "    return num/denom\n",
    "\n",
    "\n",
    "def naive_est(res):\n",
    "    treat_res = [elm[0] for elm in res[0]]\n",
    "    control_res = [elm[1] for elm in res[0]]\n",
    "    return np.mean(treat_res) - np.mean(control_res)\n",
    "\n",
    "\n",
    "def dim_est(obs_T, obs_C):\n",
    "    n1,n0 = len(obs_T), len(obs_C)\n",
    "    return np.mean(obs_T) -np.mean(obs_C), np.sqrt(np.var(obs_T)/n1 + np.var(obs_C) / n0)\n",
    "\n",
    "\n",
    "def point_est(all_treat_array, all_control_array):\n",
    "    mus_T, mus_C  = all_treat_array[:, 11:21], all_control_array[:,11:21]\n",
    "    p_T, p_C  = all_treat_array[:, 21:], all_control_array[:,21:]\n",
    "    return np.mean(np.sum((mus_T * (p_T - p_C)), axis = 1 ))\n",
    "\n",
    "\n",
    "def naive_dim_estimate(vector_T, vector_C):\n",
    "    return np.mean(vector_T) - np.mean(vector_C), np.var(vector_T)/len(vector_T) + np.var(vector_C) / len(vector_C)\n",
    "\n",
    "def is_invertible(matrix):\n",
    "    return np.linalg.det(matrix) != 0\n",
    "\n",
    "def debias_estimator(Hfuncs, debias_terms):\n",
    "    score_functions = Hfuncs - debias_terms \n",
    "    undebiased_estimator = np.mean(Hfuncs)\n",
    "    debiased_estimator = np.mean(score_functions)\n",
    "    variance_estimator = np.mean((score_functions - debiased_estimator)**2) / len(score_functions)\n",
    "    return debiased_estimator, variance_estimator, undebiased_estimator \n",
    "\n",
    "\n",
    "def dim_est_IPW(obs_T, obs_C, treated_probability, Q):\n",
    "    n1,n0 = len(obs_T), len(obs_C)\n",
    "    tau1 = np.sum(obs_T) / (Q*treated_probability)\n",
    "    tau0 = np.sum(obs_C)/(Q * (1-treated_probability))\n",
    "    estimate = tau1 - tau0\n",
    "    # var = np.sum((obs_T - tau1)**2)/ Q/treated_probability**2  + np.sum((obs_C - tau0)**2)/ Q/(1-treated_probability)**2 \n",
    "    var = (np.sum((obs_T / treated_probability - estimate) ** 2) + np.sum(( - obs_C / (1-treated_probability) - estimate) ** 2)) / Q\n",
    "    return estimate, var\n",
    "\n",
    "\n",
    "def aggregate_probability_exposure_examination(probs_matrix, exposure_matrix): \n",
    "    aggregate_probs = np.mean(probs_matrix, axis = 0)\n",
    "    exposure_probs = np.mean(exposure_matrix, axis = 0)\n",
    "    ## Euclidean distance \n",
    "    euc_dist = np.linalg.norm(aggregate_probs - exposure_probs)\n",
    "    ## NDCG LOSS \n",
    "    y_true = tf.ragged.constant(exposure_matrix)\n",
    "    y_pred = tf.ragged.constant(probs_matrix)\n",
    "    loss_NDCG = tfr.keras.losses.ApproxNDCGLoss(ragged=True)\n",
    "    NDCG_loss_result = loss_NDCG(y_true, y_pred).numpy()\n",
    "    return euc_dist, aggregate_probs, exposure_probs, NDCG_loss_result\n",
    "\n",
    "def bootstrap_evaluation(probs_mat, expos_mat, B = 100):\n",
    "    nrows = probs_mat.shape[0]\n",
    "    kl_div_list, NDCG_list = [], []\n",
    "    for b in range(B):\n",
    "        indices = np.random.choice(nrows, size=nrows, replace=True)\n",
    "        probs_mat_b = probs_mat[indices,:]\n",
    "        expos_mat_b = expos_mat[indices,:]\n",
    "\n",
    "        _, agg_probs_b, agg_expos_b, NDCG_b = aggregate_probability_exposure_examination(probs_mat_b, expos_mat_b)\n",
    "        kl_divergence_b = kl_div(agg_probs_b,agg_expos_b).sum()\n",
    "\n",
    "        kl_div_list += [kl_divergence_b]\n",
    "        NDCG_list += [NDCG_b]\n",
    "\n",
    "    return kl_div_list, NDCG_list \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Modifying the tensor for 3d input \n",
    "class MyModel_multiple(Model):\n",
    "    def __init__(self, k, num_treats,predictY=False):\n",
    "        super(MyModel_multiple, self).__init__()\n",
    "        self.k = k\n",
    "        self.num_treats = num_treats\n",
    "        self.groupNames = ['A'] + ['B' + str(i+1) for i in range(self.num_treats)]\n",
    "        \n",
    "        self.baseline_logit = Dense( 1, activation = \"linear\")\n",
    "        self.logit_dense_layer = {} \n",
    "        for g in self.groupNames:\n",
    "            self.logit_dense_layer[g] = Dense(1, activation = \"linear\")\n",
    "        self.common_hidden = Dense(3, activation = \"linear\")\n",
    "        self.softmax =tf.keras.activations.softmax\n",
    "        \n",
    "        self.predictY = predictY\n",
    "        self.d5 = Dense(self.k, activation = \"relu\")\n",
    "        self.doutcome = Dense(1)\n",
    "        \n",
    "        self.d_onehot = tf.keras.layers.Lambda(lambda x: tf.one_hot(tf.argmax(x, axis=-1), k))\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        split_structure =  [4] + [1] * self.num_treats + [1]\n",
    "        splitted_elements = tf.split(inputs, split_structure, axis=2)\n",
    "        x1 = splitted_elements[0]\n",
    "        ypredicts = splitted_elements[-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Step 1: Reshape the input \n",
    "        \n",
    "        reshape_x1 = tf.reshape(x1, (-1, x1.shape[-1]))\n",
    "        \n",
    "        ## Step 2: a common hidden layer\n",
    "        x1_common_hidden = self.common_hidden(reshape_x1)\n",
    "        \n",
    "        #x1_common_hidden_3d = tf.reshape(x1_common_hidden, [x1.shape[0],self.k, x1_common_hidden.shape[1]])\n",
    "        x1_common_hidden_3d = tf.reshape(x1_common_hidden,[-1, k, 3])\n",
    "        \n",
    "        ## Baseline logit\n",
    "        x1_final = self.baseline_logit(x1_common_hidden_3d)\n",
    "        \n",
    "        # Get the first element of the second dimension\n",
    "        # first_element = x1_final[:, 0:1, :]\n",
    "\n",
    "        # Subtract the first element from every element of the second dimension\n",
    "        # x1_final = x1_final - first_element\n",
    "\n",
    "        \n",
    "        ## Step 3: logit model\n",
    "        for i in range(self.num_treats):\n",
    "            w_g = splitted_elements[i + 1]\n",
    "            xg_hidden = self.logit_dense_layer['B'+str(i+1)](x1_common_hidden_3d)\n",
    "            x1_final = tf.add(tf.multiply(w_g, xg_hidden), x1_final)\n",
    "            \n",
    "        ## Step 4: Softmax\n",
    "        reshaped_data = tf.squeeze(x1_final, axis=-1)\n",
    "        softmax_p =  self.softmax(reshaped_data)\n",
    "\n",
    "        # for g in self.groupNames:\n",
    "        #     if g != 'A':\n",
    "        #         w_g = self.treatment_matrix_dict[g]\n",
    "        #         x = tf.math.add(tf.multiply(w_g, self.logit_dense_layer[g](x1)), x)\n",
    "        #x = tf.add(tf.multiply(self.treatment_matrix_dict['B1'],  x2_hidden), x1_hidden)\n",
    "\n",
    "        # x = tf.mul( w, x1_hidden)\n",
    "\n",
    "        y1 = self.d_onehot(softmax_p)\n",
    "        \n",
    "        if self.predictY:\n",
    "            x5 = self.d5(x1)\n",
    "            y2 = self.doutcome(tf.multiply(softmax_p, x5))\n",
    "        else:\n",
    "            \n",
    "            ypredicts = splitted_elements[-1]\n",
    "            ypredicts = tf.squeeze(ypredicts, axis=-1)\n",
    "            \n",
    "            y2 = tf.reduce_sum(tf.multiply(softmax_p, ypredicts), axis = 1 )\n",
    "            y2 = tf.expand_dims(y2, axis=-1)\n",
    "\n",
    "            \n",
    "        # ## One hot vector  \n",
    "        # y1 = softmax_p\n",
    "        # x5 = self.d5(ypredicts)\n",
    "        # print(\"softmax shape\", y1.shape)\n",
    "        \n",
    "        # ## Outcome \n",
    "    \n",
    "        # y2 = self.doutcome(tf.multiply(y1, x5))\n",
    "        \n",
    "\n",
    "\n",
    "        res = tf.concat([softmax_p,y2,softmax_p], axis=1)\n",
    "        return res\n",
    "\n",
    "# Define custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "\n",
    "    y1_true, y2_true = tf.split(y_true, [K, 1], axis=1)\n",
    "    _, y2_pred, y1_pred = tf.split(y_pred, [K, 1, K], axis=1)\n",
    "\n",
    "\n",
    "    loss1 = tf.keras.losses.CategoricalCrossentropy()(y1_true, y1_pred)\n",
    "    loss2 = tf.keras.losses.MeanSquaredError()(y2_true, y2_pred)\n",
    "    return loss1 + loss2 \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "## Helper functions \n",
    "\n",
    "def debias_estimator_new(Hfuncs, debias_terms,tau_hat):\n",
    "    psi_functions = Hfuncs - debias_terms \n",
    "    undebiased_estimator = np.mean(Hfuncs)\n",
    "    debiased_estimator = np.mean(psi_functions)\n",
    "    variance_estimator = np.sum((psi_functions - tau_hat)**2) /len(psi_functions)\n",
    "    return debiased_estimator, variance_estimator, undebiased_estimator \n",
    "\n",
    "\n",
    "def undebias_estimator_new(Hfuncs,tau_hat):\n",
    "    psi_functions = Hfuncs \n",
    "    undebiased_estimator = np.mean(Hfuncs)\n",
    "    variance_estimator = np.sum((psi_functions - tau_hat)**2) /len(psi_functions)\n",
    "    \n",
    "    return undebiased_estimator, variance_estimator \n",
    "\n",
    "\n",
    "def is_invertible(matrix):\n",
    "    return np.linalg.det(matrix) != 0\n",
    "\n",
    "\n",
    "    \n",
    "def permute_treatment_dict(J, L):\n",
    "    perm_dict = {}\n",
    "    for j in range(J):\n",
    "        perm_dict[j] = np.random.choice(L+1, 1)\n",
    "    return perm_dict\n",
    "\n",
    "## Helper function for cross validation\n",
    "def generate_indices(n, K):\n",
    "    ## Split original sample of size n into K sets \n",
    "    indices = np.linspace(0, n, K+1, dtype=int)\n",
    "    return list(zip(indices[:-1], indices[1:]))\n",
    "\n",
    "\n",
    "def train_test_split(input_data, all_inds, kth_test):\n",
    "    \n",
    "    training_ind = [all_inds[i] for i in range(len(all_inds)) if i != kth_test]\n",
    "    test_start, test_end = all_inds[kth_test]\n",
    "    if not tf.is_tensor(input_data):\n",
    "        training_data = np.concatenate([input_data[elm[0]:elm[1]] for elm in training_ind])\n",
    "    else:\n",
    "        \n",
    "        training_data = tf.concat([input_data[elm[0]:elm[1]] for elm in training_ind], axis = 0)\n",
    "    testing_data = input_data[test_start:test_end]\n",
    "    return training_data, testing_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f4cea1d-4f44-4160-9f11-88e27714f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "ith_treatment = 0 \n",
    "K = 15\n",
    "k = 15\n",
    "ith_outcome = 1\n",
    "L = 1\n",
    "ith_treat = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0338b43a-bb80-4d3a-b383-173371949307",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec0aa44-0495-49c8-8e61-bac569cb2296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 08:31:31.975994: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-06-18 08:31:31.976061: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-06-18 08:31:31.976084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (VM-209-192-centos): /proc/driver/nvidia/version does not exist\n",
      "2024-06-18 08:31:31.976609: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# model_memories = {}\n",
    "# for f in range(n_folds):\n",
    "#     model_f = MyModel_multiple(K,L)\n",
    "#     model_f.compile(loss=custom_loss,optimizer=tf.keras.optimizers.Adam())\n",
    "#     model_memories[f] = model_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc03c0-cfb8-4689-891a-ef6b1cbfd4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29781b51-d810-435b-ba0a-35cf2aa2e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fromDate = '20240527'\n",
    "toDate = '20240604'\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import requests\n",
    "\n",
    "def date_range(start, end):\n",
    "    delta = end - start  # as timedelta\n",
    "    days = [start + timedelta(days=i) for i in range(delta.days + 1)]\n",
    "    return days\n",
    "\n",
    "def form_ds(y,m,d):\n",
    "    yyyy = str(y)\n",
    "    if m < 10:\n",
    "        mm = '0' + str(m)\n",
    "    else:\n",
    "        mm = str(m)\n",
    "    if d < 10:\n",
    "        dd = '0' + str(d)\n",
    "    else:\n",
    "        dd = str(d)\n",
    "    return yyyy+mm+dd\n",
    "date_range_obj = date_range(date(int(fromDate[0:4]), int(fromDate[4:6]), int(fromDate[6:8])), date(int(toDate[0:4]), int(toDate[4:6]), int(toDate[6:8])))\n",
    "date_range_p_list = ['p_' + form_ds(d.year,d.month ,d.day) for d in date_range_obj]\n",
    "date_range_loghour_list = []\n",
    "for date_p in date_range_p_list:\n",
    "    for hr in range(24):\n",
    "        if hr < 10:\n",
    "            \n",
    "            date_range_loghour_list += [date_p + '0' +str(hr)]\n",
    "        else:\n",
    "            date_range_loghour_list += [date_p + str(hr)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aeda60-2c54-4ba2-a40c-54995373094c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752cef6-1aa8-4495-953a-6662328726a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3b0d4-3a7f-401a-871c-255b2565f2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84fc09e1-88cf-43bb-8655-0da8173ffd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlGroupIDs = [92028784]\n",
    "treatGroupIDs = [92028785]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46aeffe-db21-44e8-8b9f-42eceefa0530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95dde8a-fd43-4536-9946-7bcafab6ffbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7bf206-9454-46b4-9ff9-bb65610f77fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2d460-827f-44bb-b20e-c170e8546f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1077d8-8d1f-461c-9280-e7e801b79e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9941a-c60f-4100-9e48-240d6d09af23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec508a-0ea2-4399-b8cb-1ae884b52980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d6af0-cf5f-472e-bf41-6c632ee8548f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a300eea-23ea-4a73-ba65-719261a2e5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762297b1-8c74-48a3-951c-e4988afa7ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff881c-a78d-47f2-8b03-163f2d04762c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e80fae-875b-4b9a-b4b4-b4b9dd72c8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d349aec-10a7-470c-8ff8-954490dbfe4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc897459-7c4f-430d-8e0f-7b87d8e20673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885928b-d1d2-4d74-9530-71de5f691881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148df54f-2692-461c-8742-212859208585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d600f-0d19-4ab8-bf59-9e39db0f0c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c391f9a3-c337-4b08-bbe0-ea3fdb78a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_value_gradient_subgroup(predict_p_treat, predict_outcome_treat, predict_p_control, predict_outcome_control, J):\n",
    "    # J: dimension K, indicator function of whether item k belongs to the subgroup\n",
    "    Ey1 = np.sum(predict_p_treat * predict_outcome_treat * J, axis=1, keepdims=True)\n",
    "    Ey0 = np.sum(predict_p_control * predict_outcome_control * J, axis=1, keepdims=True)\n",
    "    dHdtheta0 = predict_p_treat * (predict_outcome_treat * J - Ey1) - predict_p_control * (predict_outcome_control * J - Ey0)\n",
    "    dHdtheta0 = dHdtheta0[:, 1:]\n",
    "    dHdtheta1 = predict_p_treat * (predict_outcome_treat * J - Ey1) \n",
    "    dHdmu = (predict_p_treat - predict_p_control) * J\n",
    "    gradient_vector_H = np.concatenate([dHdtheta0, dHdtheta1, dHdmu], axis =1 )\n",
    "    return gradient_vector_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67d7d672-8259-41db-8629-f493d21ecb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_groups = ['f4_51到100元','f5_101到500元','f3_21到50元','f1_<=10元']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e1baf11-7fa2-4f43-b5cc-85ae04d013dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4716c347-eb27-48da-b113-dde7104d6a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model training \n",
    "for hr in range(len(date_range_loghour_list)):\n",
    "    print(\"Start for hour\", hr)\n",
    "    dh = hr\n",
    "    provider_weixin_experiment.table(\"\", priParts = [date_range_loghour_list[dh]]).createOrReplaceTempView(\"df_vv_details\")\n",
    "    df_spark = session.sql(f\"select * from df_vv_details\")\n",
    "    df_input = df_spark.toPandas()\n",
    "    print(\"Finish loading the data\")\n",
    "\n",
    "\n",
    "    ## data-preprocessing for different outcomes \n",
    "    df_input['outcome1'] = df_input['max_target_action_cnt']\n",
    "    df_input['outcome2'] = df_input['bid_']\n",
    "    ## outcome3 - type1 conversion \n",
    "    df_input['outcome3'] = ((df_input['promotion_type']==1 ).astype(float)) * (df_input['max_target_action_cnt'].astype(float))\n",
    "    ## outcome4 - type2 conversion \n",
    "    df_input['outcome4'] = ((df_input['promotion_type']==2 ).astype(float)) * (df_input['max_target_action_cnt'].astype(float))\n",
    "    ## outcome5 - type1 bid \n",
    "    df_input['outcome5'] = ((df_input['promotion_type']==1 ).astype(float)) * (df_input['bid_'].astype(float))\n",
    "    ## outcome6 - type2 bid \n",
    "    df_input['outcome6'] = ((df_input['promotion_type']==2 ).astype(float)) * (df_input['bid_'].astype(float))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## data-preprocessing for different outcomes \n",
    "    df_input['outcome1_pred'] = df_input['cvr_']\n",
    "    df_input['outcome2_pred'] = df_input['bid_']\n",
    "    ## outcome3 - type1 conversion \n",
    "    df_input['outcome3_pred'] = ((df_input['promotion_type']==1 ).astype(float)) * (df_input['cvr_'].astype(float))\n",
    "    ## outcome4 - type2 conversion \n",
    "    df_input['outcome4_pred'] = ((df_input['promotion_type']==2).astype(float))  * (df_input['cvr_'].astype(float))\n",
    "    ## outcome5 - type1 bid \n",
    "    df_input['outcome5_pred'] = ((df_input['promotion_type']==1 ).astype(float))  * (df_input['bid_'].astype(float))\n",
    "    ## outcome6 - type2 bid \n",
    "    df_input['outcome6_pred'] = ((df_input['promotion_type']==2 ).astype(float))  * (df_input['bid_'].astype(float))\n",
    "                                                                                   \n",
    "\n",
    "    df_sellerside = df_input[df_input['groupid'].isin([92028784,92028785])]\n",
    "\n",
    "    df_twoside = df_input[~df_input['groupid'].isin([92028784,92028785])]\n",
    "    df_twoside.to_csv(f\"prediction_result_new/twoside_data{hr}.csv\")\n",
    "\n",
    "    exposed_twoside = df_twoside.loc[df_twoside['selected']==1,:]\n",
    "    model_memories = {}\n",
    "    for f in range(n_folds):\n",
    "        model_f = MyModel_multiple(K,L)\n",
    "        model_f.compile(loss=custom_loss,optimizer=tf.keras.optimizers.Adam())\n",
    "        model_memories[f] = model_f\n",
    "\n",
    "    ## Result from ground-truth experiment \n",
    "    groundtruth_outcome_dict = {}\n",
    "    for i in range(6):\n",
    "        twoside_T, twoside_C = exposed_twoside.loc[exposed_twoside['groupid']==92028783,f\"outcome{i+1}\"],exposed_twoside.loc[exposed_twoside['groupid'].isin([92028781,92028782]),f\"outcome{i+1}\"]\n",
    "        dim_naive_i = dim_est_naive(twoside_T, twoside_C)\n",
    "        dim_ipw_i = dim_est_IPW(twoside_T,twoside_C, 0.5, len(twoside_T) + len(twoside_C))\n",
    "        groundtruth_outcome_dict[i] = [dim_naive_i,dim_ipw_i, len(twoside_T), len(twoside_C)]\n",
    "        with open(f\"prediction_result_new/groundtruth_outcome{i+1}_{dh}.pickle\", 'wb') as ff:\n",
    "            pickle.dump( groundtruth_outcome_dict, ff)\n",
    "    print(\"Finish writing ground truth for \", dh)\n",
    "\n",
    "    ## Covariates\n",
    "    base_bid_table = df_sellerside.pivot_table(index='req_id_', columns = 'row_number', values = 'bid_')\n",
    "    sort_score_table = df_sellerside.pivot_table(index='req_id_', columns = 'row_number', values = 'sort_score_')\n",
    "    cvr_table = df_sellerside.pivot_table(index='req_id_', columns = 'row_number', values = 'cvr_')\n",
    "    ecpm_table = df_sellerside.pivot_table(index='req_id_', columns = 'row_number', values = 'ecpm_')\n",
    "    covariates_table = df_sellerside.loc[:, ['req_id_','quota_level','row_number']].pivot_table(index='req_id_', columns = 'row_number', values= 'quota_level', aggfunc=lambda x: x)\n",
    "    x_covariates = covariates_table.values\n",
    "    all_inds = generate_indices(base_bid_table.shape[0], n_folds)\n",
    "\n",
    "    ## Tables \n",
    "    ## Covariates\n",
    "    outcome_table_dict = {} \n",
    "    prediction_table_dict = {} \n",
    "    outcome_tensor_dict = {} \n",
    "    prediction_tensor_dict = {}\n",
    "    outcome_vector_dict = {} \n",
    "    outcome_vector_tensor_dict = {} \n",
    "    for i in range(6):\n",
    "        outcome_i_table = df_sellerside.pivot_table(index='req_id_', columns = 'row_number', values = f\"outcome{i+1}\")\n",
    "        outcome_table_dict[i+1] = outcome_i_table\n",
    "\n",
    "        outcome_i_tensor = tf.convert_to_tensor(outcome_i_table.fillna(0).values, dtype = float)\n",
    "        outcome_tensor_dict[i+1] = outcome_i_tensor\n",
    "\n",
    "        pred_i_table = df_sellerside.pivot_table(index='req_id_', columns = 'row_number', values = f\"outcome{i+1}_pred\")\n",
    "        prediction_table_dict[i+1] = pred_i_table\n",
    "\n",
    "        pred_i_tensor = tf.convert_to_tensor(pred_i_table.fillna(0).values, dtype = float)\n",
    "        prediction_tensor_dict[i+1] = pred_i_tensor\n",
    "\n",
    "\n",
    "        outcome_i_vec = df_sellerside.loc[df_sellerside['selected']==1, ['req_id_', f\"outcome{i+1}\"]].pivot_table(index='req_id_', values= f\"outcome{i+1}\")\n",
    "        outcome_vector_dict[i+1] = outcome_i_vec\n",
    "\n",
    "        outcome_i_vec_tensor = tf.convert_to_tensor(outcome_i_vec.values, dtype = float)\n",
    "        outcome_vector_tensor_dict[i+1] = outcome_i_vec_tensor\n",
    "\n",
    "    ## Exposure indicator \n",
    "\n",
    "    is_selected_table = df_sellerside.pivot_table(index='req_id_', columns = 'row_number', values = 'selected')\n",
    "    is_selected_indicator = is_selected_indicator = tf.convert_to_tensor(is_selected_table.fillna(0).values, dtype = float)\n",
    "    exposure_matrix = is_selected_table.values \n",
    "    ## Convert to tensor \n",
    "    x_sort_score = tf.convert_to_tensor(sort_score_table.fillna(0).values, dtype = float)\n",
    "    x_basebid = tf.convert_to_tensor(base_bid_table.fillna(0).values, dtype=float)\n",
    "    x_ecpm = tf.convert_to_tensor(ecpm_table.fillna(0).values, dtype = float)\n",
    "    x_cvr = tf.convert_to_tensor(cvr_table.fillna(0).values, dtype = float)\n",
    "    groupid_table = df_sellerside.pivot_table(index='req_id_', columns = 'row_number', values = 'groupid')\n",
    "    w_dict = {}\n",
    "    for i in range(len(treatGroupIDs)):\n",
    "        w_dict[i] = tf.convert_to_tensor(groupid_table.values == treatGroupIDs[i], dtype = float)\n",
    "    \n",
    "    for i in range(6):\n",
    "        \n",
    "        ith_outcome = i + 1 \n",
    "        print(\"Start outcome for \", ith_outcome)\n",
    "        inputs_3d = tf.stack([x_basebid, x_sort_score,x_ecpm, x_cvr] + [w_dict[v] for v in range(len(treatGroupIDs))] + [prediction_tensor_dict[ith_outcome]], axis = 2)\n",
    "        exposure_indicator_outcome = tf.concat([is_selected_indicator, outcome_vector_tensor_dict[ith_outcome]], axis = 1)\n",
    "\n",
    "\n",
    "        samp_size = exposure_indicator_outcome.shape[0]\n",
    "        w_all_treat = tf.convert_to_tensor(np.array([[1] * k for _ in range(samp_size)],dtype='float32'))\n",
    "        w_all_control = tf.convert_to_tensor(np.array([[0] * k for _ in range(samp_size)],dtype='float32'))\n",
    "\n",
    "        inputs_all_treat_3d = tf.stack([x_basebid, x_sort_score,x_ecpm, x_cvr] +  [w_all_treat if l == ith_treat else w_all_control for l in range(L)]+ [prediction_tensor_dict[ith_outcome]], axis = 2)\n",
    "        inputs_all_control_3d = tf.stack([x_basebid, x_sort_score,x_ecpm, x_cvr] + [w_all_control for l in range(L)] + [prediction_tensor_dict[ith_outcome]], axis = 2)\n",
    "        inputs_all_treat_3d = tf.cast(inputs_all_treat_3d, dtype = 'float32')\n",
    "        inputs_all_control_3d = tf.cast(inputs_all_control_3d, dtype = 'float32')\n",
    "        inputs_all_treat_3d_dict = {} \n",
    "        for l in range(L):\n",
    "            inputs_all_treat_3d_l = tf.stack([x_basebid, x_sort_score,x_ecpm, x_cvr] + [w_all_treat if v == l else w_all_control for v in range(L)] + [prediction_tensor_dict[ith_outcome]], axis = 2)\n",
    "            inputs_all_treat_3d_dict[l] = tf.cast(inputs_all_treat_3d_l, dtype = 'float32')\n",
    "        inputs_all_treat_3d_dict = {} \n",
    "        for l in range(L):\n",
    "            inputs_all_treat_3d_l = tf.stack([x_basebid, x_sort_score,x_ecpm, x_cvr] + [w_all_treat if v == l else w_all_control for v in range(L)] + [prediction_tensor_dict[ith_outcome]], axis = 2)\n",
    "            inputs_all_treat_3d_dict[l] = tf.cast(inputs_all_treat_3d_l, dtype = 'float32')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## Modify the code \n",
    "        num_features= 4\n",
    "        hfuncs_each_fold,  debias_terms_each_fold, probs_mat_each_fold, expos_mat_each_fold = {},{},{},{}\n",
    "        eigen_value_each_fold = {} \n",
    "        history_each_fold = {}\n",
    "        \n",
    "        for f in range(n_folds):\n",
    "\n",
    "            print(\"Start fold \", f )\n",
    "            f_start, f_end = all_inds[f]\n",
    "\n",
    "            xcov_train, xcov_test = train_test_split(x_covariates, all_inds, f)\n",
    "            inputs_3d_train, inputs_3d_test = train_test_split(inputs_3d, all_inds, f)\n",
    "            exposure_indicator_outcome_train, exposure_indicator_outcome_test = train_test_split(exposure_indicator_outcome, all_inds, f)\n",
    "            inputs_all_treat_3d_train, inputs_all_treat_3d_test = train_test_split(inputs_all_treat_3d, all_inds, f)\n",
    "            inputs_all_control_3d_train, inputs_all_control_3d_test = train_test_split(inputs_all_control_3d, all_inds, f)\n",
    "            is_selected_indicator_train,is_selected_indicator_test = train_test_split(is_selected_indicator, all_inds, f)\n",
    "            treat_control_dict = {} \n",
    "            for l in range(L):\n",
    "                inputs_3d_train_l,inputs_3d_test_l  = train_test_split(inputs_all_treat_3d_dict[l], all_inds, f)\n",
    "\n",
    "                treat_control_dict[l] = {'train':inputs_3d_train_l, 'test': inputs_3d_test_l}\n",
    "            xcvr_train, xcvr_test =train_test_split(x_cvr, all_inds, f)\n",
    "            myModelMultiple = model_memories[f]\n",
    "            if ith_outcome == 1:\n",
    "                history_f = myModelMultiple.fit(inputs_3d_train, exposure_indicator_outcome_train,epochs=1000,verbose=False)\n",
    "                history_each_fold[f] = history_f \n",
    "                model_memories[f] = myModelMultiple\n",
    "            else:\n",
    "                myModelMultiple = model_memories[f]\n",
    "\n",
    "            res_tempt = np.array(myModelMultiple.predict(inputs_all_treat_3d_test, verbose=False)) - np.array(myModelMultiple.predict(inputs_all_control_3d_test, verbose=False))\n",
    "            exposure_indicator_array = is_selected_indicator_test\n",
    "            model_pred_H = np.array(myModelMultiple.predict(inputs_3d_test, verbose=False))\n",
    "            pred_H_new = np.array(myModelMultiple.predict(inputs_all_treat_3d_test, verbose=False)) - np.array(myModelMultiple.predict(inputs_all_control_3d_test, verbose= False))\n",
    "\n",
    "            exposure_indicator_array = is_selected_indicator_test\n",
    "            model_pred_all_treat = myModelMultiple.predict(inputs_all_treat_3d_test, verbose=False)\n",
    "            model_pred_all_control = myModelMultiple.predict(inputs_all_control_3d_test, verbose=False)\n",
    "            all_treat_array, all_control_array = np.array(model_pred_all_treat), np.array(model_pred_all_control)\n",
    "\n",
    "            counterfactual_pred_dict = {} \n",
    "            for l in range(L):\n",
    "                counterfactual_pred_dict[l] = myModelMultiple.predict(treat_control_dict[l]['test'], verbose=False)\n",
    "            ## Outcome - prediction model \n",
    "            indicator_bool = tf.cast(is_selected_indicator_train, dtype=tf.bool)\n",
    "            selected_elements = tf.boolean_mask(inputs_3d_train[:,:,:num_features], indicator_bool)\n",
    "            x_prediction_train,x_prediction_test = train_test_split(prediction_tensor_dict[ith_outcome], all_inds, f)\n",
    "            mus_T, mus_C  = x_prediction_test,x_prediction_test\n",
    "\n",
    "            p_T, p_C  = all_treat_array[:, :K], all_control_array[:,:K]\n",
    "\n",
    "            outcomes_train, outcomes_test = train_test_split(outcome_vector_dict[ith_outcome].values, all_inds, f)\n",
    "            rewards_array = outcomes_test\n",
    "            Ey1,Ey0 = np.sum(mus_T * p_T, axis = 1), np.sum(mus_C * p_C, axis = 1)\n",
    "            pv1,pv0 = np.sum(exposure_indicator_array * p_T, axis = 1), np.sum(exposure_indicator_array * p_C, axis = 1)\n",
    "            rewards_uv = np.sum(exposure_indicator_array * rewards_array, axis = 1 )\n",
    "\n",
    "            treatment_indicator_array = 1 * (np.array(w_dict[ith_treat])[f_start:f_end,:])\n",
    "            pv_given_uvw = p_T * treatment_indicator_array + p_C * (1 - treatment_indicator_array)\n",
    "            ## FIX: get the realized probabilities \n",
    "            p_realized = model_pred_H[:,:K]\n",
    "\n",
    "            \n",
    "            ## 1. COMPUTE THE GRADIENT OF LOSSS  \n",
    "            ## FIX: change to realized outcome \n",
    "            #dl1dtheta0 = pv_given_uvw - exposure_indicator_array\n",
    "            dl1dtheta0 = p_realized - exposure_indicator_array\n",
    "            dl1dtheta0 = dl1dtheta0[:, 1:] \n",
    "            ## FIX: iterate over all L \n",
    "            dl1dthetal_dict = {} \n",
    "            for l in range(L):\n",
    "                treatment_indicator_array_l = w_dict[l][f_start:f_end, :]\n",
    "                dl1dthetal_dict[l] = treatment_indicator_array_l *  (p_realized - exposure_indicator_array)\n",
    "            dl2dmu = exposure_indicator_array * (mus_T -rewards_array)\n",
    "            gradient_vector_l = np.concatenate([dl1dtheta0]+[dl1dthetal_dict[l] for l in range(L)] +[dl2dmu], axis =1 )\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ## 2. COMPUTE  THE GRADIENT OF H FUNCTION\n",
    "            \n",
    "            ## Need dictionary for each \n",
    "            \n",
    "            dHdtheta0 = p_T * (mus_T - Ey1.reshape(mus_T.shape[0],1)) - p_C * (mus_C - Ey0.reshape(mus_C.shape[0],1))\n",
    "            dHdtheta0 = dHdtheta0[:, 1:]\n",
    "\n",
    "\n",
    "\n",
    "            ## FIX: iterate over each l \n",
    "            dHdthetal_dict = {} \n",
    "            for l in range(L):\n",
    "\n",
    "                p_T_thetal = counterfactual_pred_dict[l][:,:K]\n",
    "                Eyl = np.sum(mus_T * p_T_thetal, axis = 1)\n",
    "                dHdthetal_dict[l] = p_T_thetal * (mus_T - Eyl.reshape(mus_T.shape[0],1))\n",
    "                ## 0 for the groups that are not the target treatment group \n",
    "                if l != ith_treat:\n",
    "                    dHdthetal_dict[l] = 0 * (p_T_thetal * (mus_T - Eyl.reshape(mus_T.shape[0],1)))\n",
    "\n",
    "            #dHdthetal = p_T * (mus_T - Ey1.reshape(mus_T.shape[0],1))\n",
    "            dHdmu = p_T - p_C\n",
    "            #gradient_vector_H = np.concatenate([dHdtheta0,dHdthetal,dHdmu], axis =1 )\n",
    "\n",
    "            ## FIX: iterate over all l \n",
    "            gradient_vector_H = np.concatenate([dHdtheta0]+[dHdthetal_dict[l] for l in range(L)]+[dHdmu], axis =1 )\n",
    "            ## Gradient over all other treatments\n",
    "            \n",
    "            ## get gradient_vector_H for each category \n",
    "            dict_gradient_vector_H = {} \n",
    "            for c_group in cate_groups:\n",
    "                J_indicator = (xcov_test == c_group) * 1 \n",
    "                dict_gradient_vector_H[c_group] = compute_value_gradient_subgroup(p_T, mus_T, p_C, mus_C, J_indicator)\n",
    "\n",
    "\n",
    "            ## 3. FIND THE EXPECTATION OF HESSIAN MATRIX \n",
    "\n",
    "\n",
    "\n",
    "            Hessian_all = np.zeros((inputs_3d_test.shape[0],(L+2) * K - 1,  (L+2) * K - 1))\n",
    "\n",
    "            montecarlo_expected_probability = np.zeros(exposure_indicator_array.shape)\n",
    "\n",
    "            selected_indicator_dict  = {}\n",
    "            assignment_pd_dict = {} \n",
    "            dmu_dict = {} \n",
    "\n",
    "\n",
    "\n",
    "            M = 500\n",
    "            for m in range(M):\n",
    "                unique_qids = pd.DataFrame({\"quota_id_\": df_sellerside['quota_id_'].unique()})\n",
    "\n",
    "                ## Need to modify here if multiple treatment \n",
    "                unique_qids['assignment'] = np.random.binomial(1, 0.5,unique_qids.shape[0])\n",
    "                df_assignment_m = df_sellerside.loc[:,['quota_id_', 'req_id_','row_number']].merge(unique_qids, on = \"quota_id_\")\n",
    "                W_matrix_m = df_assignment_m.pivot_table(index=\"req_id_\", columns=\"row_number\", values=\"assignment\").values\n",
    "                w_dict_m = {0:tf.convert_to_tensor(W_matrix_m == 1, dtype = float) }\n",
    "                inputs_3d_m = tf.stack([x_basebid, x_sort_score,x_ecpm, x_cvr] + [tf.convert_to_tensor(W_matrix_m == 1, dtype = float)] + [prediction_tensor_dict[ith_outcome]], axis = 2)\n",
    "\n",
    "                inputs_3d_test_m = inputs_3d_m[f_start:f_end,:]\n",
    "                model_pred_m = np.array(myModelMultiple.predict(inputs_3d_test_m, verbose=False))[:,:K]\n",
    "                outer_product_pv1pv2 = np.array([np.outer(row_[1:], row_[1:]) for row_ in model_pred_m])\n",
    "                outer_product_treatment_indicator = np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                outer_product_pv1_one_minus_pv2 = np.array([np.outer(row_, 1-row_) for row_ in model_pred_m])\n",
    "\n",
    "\n",
    "\n",
    "                is_selected_indicator_test = np.array(exposure_matrix[f_start:f_end,:])\n",
    "                # selected_indicator_dict[m] = is_selected_indicator_test \n",
    "                d2l2dtheta0 = - np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "\n",
    "                ## FIX: Iterate over l \n",
    "                d2l2dthetal_dict = {}\n",
    "                for l in range(L):\n",
    "                    ## K by K \n",
    "\n",
    "                    w_m_l = np.array(w_dict_m[l][f_start:f_end,:])\n",
    "\n",
    "                    ## Off-diagonal terms \n",
    "                    d2l2dtheta1 =  -np.array([np.outer(row_, row_) for row_ in w_m_l]) * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                    # d2l2dtheta1 = - p_treat * p_treat * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                    ## Modify diagonal terms\n",
    "                    for i in range(d2l2dtheta1.shape[0]):\n",
    "                        treat_indicator_i = w_m_l[i,:]\n",
    "                        probs_i = model_pred_m[i,:]\n",
    "                        np.fill_diagonal(d2l2dtheta1[i],treat_indicator_i * probs_i * (1-probs_i))\n",
    "                        # np.fill_diagonal(d2l2dtheta1[i], p_treat * probs_i * (1-probs_i))\n",
    "                    d2l2dthetal_dict[l] = d2l2dtheta1\n",
    "\n",
    "\n",
    "                ## FIX: iterate over all l1, l2 \n",
    "                d2ldthetal1dthetal2 = {} \n",
    "                for l in range(L):\n",
    "                    w_m_l = np.array(w_dict_m[l][f_start:f_end,:])\n",
    "                    ## Off-diagonal terms \n",
    "                    ## !!!!!!!!!!!!!!!!! Ruohan: please double check the following, I modified the original one.\n",
    "                    d2l2dtheta0dtheta1 = - np.multiply(w_m_l[:,np.newaxis, :], np.array([np.outer(row_, row_) for row_ in model_pred_m]))\n",
    "                    # d2l2dtheta0dtheta1 = - p_treat * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                    for i in range(d2l2dtheta0dtheta1.shape[0]):\n",
    "                        treat_indicator_i = w_m_l[i,:]\n",
    "                        p_1minusp_i = model_pred_m[i,:] * (1 - model_pred_m[i,:])\n",
    "                        np.fill_diagonal(d2l2dtheta0dtheta1[i], treat_indicator_i * p_1minusp_i)\n",
    "                    ## NOTE: -1 to indicate the baseline theta \n",
    "                    d2ldthetal1dthetal2[(-1,l)] = d2l2dtheta0dtheta1[:,1:,:]\n",
    "                    d2ldthetal1dthetal2[(l,-1)] = np.transpose(d2l2dtheta0dtheta1[:,1:,:], (0,2,1))\n",
    "                    for l_prime in range(L):\n",
    "                        if l != l_prime: \n",
    "                            w_m_l = np.array(w_dict_m[l][f_start:f_end,:])\n",
    "                            w_m_l_prime = np.array(w_dict_m[l_prime][f_start:f_end,:])\n",
    "                            indicator_outer = np.array([np.outer(w_m_l[i,:], w_m_l_prime[i,:]) for i in range(w_m_l.shape[0])])\n",
    "\n",
    "                            d2l2dthetal1dthetal2 = -  indicator_outer * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                            # d2l2dthetal1dthetal2 = -  p_treat * p_treat * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                            d2ldthetal1dthetal2[(l,l_prime)]  = d2l2dthetal1dthetal2\n",
    "                            d2ldthetal1dthetal2[(l_prime,l)]  = np.transpose(d2l2dthetal1dthetal2, (0,2,1))\n",
    "                        else:\n",
    "                            d2ldthetal1dthetal2[(l,l)] = d2l2dthetal_dict[l]\n",
    "\n",
    "                d2l2dmu = np.zeros(d2l2dtheta1.shape)\n",
    "\n",
    "                for i in range(d2l2dmu.shape[0]):\n",
    "                    #p_1minusp_i = (1 - model_pred_m[i,:]) * (1 - model_pred_m[i,:])\n",
    "                    p_1minusp_i = model_pred_m[i,:] * (1 - model_pred_m[i,:])\n",
    "                    # treatment_i = treatment_indicator_array[i,:]\n",
    "                    # exposure_i = is_selected_indicator_test[i,:]\n",
    "                    np.fill_diagonal(d2l2dtheta0[i], p_1minusp_i)\n",
    "                    np.fill_diagonal(d2l2dmu[i], model_pred_m[i,:])\n",
    "\n",
    "\n",
    "                d2l2dtheta0 = d2l2dtheta0[:,1:, 1:]\n",
    "                # d2l2dtheta01_k_m_1_by_k = d2l2dtheta01[:, 1:,:]\n",
    "                # d2l2dtheta10_k_by_k_m_1 = d2l2dtheta01[:, :,1:]\n",
    "                Hessian_first_row = np.concatenate([d2l2dtheta0] + [d2ldthetal1dthetal2[(-1, l)] for l in range(L)] + [np.zeros((d2l2dtheta0.shape[0], K-1, K))], axis =2)\n",
    "\n",
    "                ## 1 to L + 1 row \n",
    "                Hessian_middle_dict = {}\n",
    "                for l in range(L):\n",
    "                    row_l = np.concatenate([d2ldthetal1dthetal2[(l, -1)]] + [d2ldthetal1dthetal2[(l, l_prime)] for l_prime in range(L)] +[np.zeros((d2l2dtheta0.shape[0], K, K))], axis =2)\n",
    "\n",
    "                    Hessian_middle_dict[l] = row_l                                                                           \n",
    "\n",
    "\n",
    "                Hessian_third_row = np.concatenate((np.zeros((d2l2dtheta0.shape[0], K, K  * (L + 1 ) - 1 )), d2l2dmu), axis =2)\n",
    "\n",
    "                Hessian = np.concatenate([Hessian_first_row] + [Hessian_middle_dict[l] for l in range(L)] + [Hessian_third_row], axis = 1 )\n",
    "\n",
    "                dmu_dict[m] = d2l2dmu\n",
    "\n",
    "                Hessian_all = Hessian_all + Hessian\n",
    "\n",
    "\n",
    "\n",
    "            Hessian_final = Hessian_all / M\n",
    "            count_finite = 0\n",
    "            score_funcs = np.zeros(len(Hessian_final))\n",
    "            count_infinite = 0\n",
    "\n",
    "            for i in range(len(Hessian_final)):\n",
    "                if is_invertible(Hessian_final[i]):\n",
    "                    try:\n",
    "                        score_funcs[i] = gradient_vector_H[i]@np.linalg.inv(Hessian_final[i])@gradient_vector_l[i]\n",
    "                        count_finite += 1 \n",
    "                    except: \n",
    "                        print(\"Fail for inversion\")\n",
    "                        count_infinite += 1\n",
    "\n",
    "\n",
    "\n",
    "            ## END OF FOR LOOP FOR EACH ITERATION OVER CROSS FITTING\n",
    "            hfuncs_f, debias_term_f = Ey1 - Ey0, score_funcs\n",
    "            hfuncs_each_fold[f] =hfuncs_f\n",
    "            debias_terms_each_fold[f] = debias_term_f\n",
    "\n",
    "\n",
    "            ## Store data for NDCG evaluation \n",
    "            expos_mat_f = is_selected_indicator_test.astype(float)\n",
    "            probs_mat_f = model_pred_H[:, :K]\n",
    "\n",
    "            probs_mat_each_fold[f] = probs_mat_f \n",
    "            expos_mat_each_fold[f] = expos_mat_f \n",
    "            result_to_save = [hfuncs_each_fold, debias_terms_each_fold,probs_mat_each_fold,expos_mat_each_fold ]\n",
    "            \n",
    "            with open(f\"prediction_result_new/outcome{ith_outcome}_{dh}_result.pickle\", 'wb') as ff:\n",
    "                pickle.dump( result_to_save, ff)\n",
    "            \n",
    "            ## SAVE CATE RESULTS\n",
    "            \n",
    "            for c_group in cate_groups:\n",
    "                gradient_vector_H_cgroup = dict_gradient_vector_H[c_group]\n",
    "                J_indicator = (xcov_test == c_group) * 1 \n",
    "                Ey1_cgroup,Ey0_cgroup =  np.sum(mus_T * p_T * J_indicator, axis = 1), np.sum(mus_C * p_C * J_indicator, axis = 1)\n",
    "                score_funcs_cgroup = np.zeros(len(Hessian_final))\n",
    "                count_infinite = 0\n",
    "\n",
    "                for i in range(len(Hessian_final)):\n",
    "                    if is_invertible(Hessian_final[i]):\n",
    "                        try:\n",
    "                            score_funcs_cgroup[i] = gradient_vector_H_cgroup[i]@np.linalg.inv(Hessian_final[i])@gradient_vector_l[i]\n",
    "                            count_finite += 1 \n",
    "                        except: \n",
    "                            print(\"Fail for inversion\")\n",
    "                            count_infinite += 1\n",
    "                hfuncs_f_cgroup, debias_term_f_cgroup = Ey1_cgroup - Ey0_cgroup, score_funcs_cgroup\n",
    "            \n",
    "                with open(f\"prediction_result_new/outcome{ith_outcome}_{dh}_result_{c_group}_fold{f}.pickle\", 'wb') as ff:\n",
    "                    pickle.dump( [hfuncs_f_cgroup, debias_term_f_cgroup], ff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d7102-2ce1-471b-8959-3920b90c4ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21689f-f647-46f6-804f-53ca9a37e452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44076181-c251-4f08-91c6-2481a3d0fed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133ed79-2abd-4aa8-b642-08644a7af4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
