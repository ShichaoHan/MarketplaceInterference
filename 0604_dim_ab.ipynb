{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Model \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from scipy.special import kl_div\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(20240528)\n",
    "import tensorflow_ranking as tfr \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Helper functions and set-ups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graphing set-up \n",
    "import seaborn as sns\n",
    "x = np.linspace(-4, 4, 100)\n",
    "tencent_blue = (0,0.3215686274509804,0.8509803921568627)\n",
    "tencent_orange = (0.9333333333333333, 0.49411764705882355, 0.2784313725490196)\n",
    "\n",
    "\n",
    "# Calculate y-values for the standard normal density curve\n",
    "y_standard_normal = (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## True Exposure Model and Data Generating Process \n",
    "def logistic_row(row):\n",
    "    return np.exp(row) / np.sum(np.exp(row))\n",
    "\n",
    "def dim_est(obs_T, obs_C, treated_probability, Q):\n",
    "    n1,n0 = len(obs_T), len(obs_C)\n",
    "    tau1 = np.sum(obs_T) / (Q*treated_probability)\n",
    "    tau0 = np.sum(obs_C)/(Q * (1-treated_probability))\n",
    "    estimate = tau1 - tau0\n",
    "    # var = np.sum((obs_T - tau1)**2)/ Q/treated_probability**2  + np.sum((obs_C - tau0)**2)/ Q/(1-treated_probability)**2 \n",
    "    var = (np.sum((obs_T / treated_probability - estimate) ** 2) + np.sum((obs_C / (1-treated_probability) - estimate) ** 2)) / Q\n",
    "    return estimate, var\n",
    "\n",
    "def DGP_new_heterogeneous(J, Q, K, promo_ratio, query_matrix, X_goodbads, X_utility, treated_probability=0.5, treat_control_pool = [True, False]):\n",
    "    ## Randomize over the treatment assignment matrix \n",
    "    treatment_dict = {}\n",
    "    for j in range(J):\n",
    "        treatment_dict[j] = np.random.choice(treat_control_pool, 1, p=[treated_probability, 1 - treated_probability])\n",
    "\n",
    "    W_matrix = []\n",
    "    for each_query in range(Q):\n",
    "         W_matrix = np.append(W_matrix, [treatment_dict[ind] for ind in query_matrix[each_query]])\n",
    "\n",
    "    outcome_noise =  np.random.normal(size=(Q, K)) \n",
    "    W_matrix = W_matrix.reshape(Q,K)\n",
    "    # W_matrix = W_matrix.reshape(Q,K)\n",
    "    final_score_matrix = W_matrix * promo_ratio * X_goodbads   + X_utility\n",
    "\n",
    "    X_logit = np.apply_along_axis(logistic_row, axis=1, arr=final_score_matrix)\n",
    "    expose_indices = np.array([np.random.choice(np.arange(K), size = 1, p = X_logit[i,:]) for i in range(Q)])\n",
    "    inddds = np.array(list(np.arange(K)) * Q).reshape(Q,K)\n",
    "    exposure_matrix = np.array([inddds[i,:] == expose_indices[i] for i in range(Q)])\n",
    "\n",
    "    ## Outcome model  \n",
    "    ## First: a true outcome model of Exponential \n",
    "    outcome_potential = X_utility\n",
    "\n",
    "    return query_matrix, X_goodbads, X_utility,W_matrix, exposure_matrix, outcome_potential, X_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions \n",
    "\n",
    "def debias_estimator_new(Hfuncs, debias_terms,tau_hat):\n",
    "    psi_functions = Hfuncs - debias_terms \n",
    "    undebiased_estimator = np.mean(Hfuncs)\n",
    "    debiased_estimator = np.mean(psi_functions)\n",
    "    variance_estimator = np.sum((psi_functions - tau_hat)**2) /len(psi_functions)\n",
    "    return debiased_estimator, variance_estimator, undebiased_estimator \n",
    "\n",
    "\n",
    "def undebias_estimator_new(Hfuncs,tau_hat):\n",
    "    psi_functions = Hfuncs \n",
    "    undebiased_estimator = np.mean(Hfuncs)\n",
    "    variance_estimator = np.sum((psi_functions - tau_hat)**2) /len(psi_functions)\n",
    "    \n",
    "    return undebiased_estimator, variance_estimator \n",
    "\n",
    "\n",
    "def is_invertible(matrix):\n",
    "    return np.linalg.det(matrix) != 0\n",
    "\n",
    "\n",
    "    \n",
    "def permute_treatment_dict(J, L):\n",
    "    perm_dict = {}\n",
    "    for j in range(J):\n",
    "        perm_dict[j] = np.random.choice(L+1, 1)\n",
    "    return perm_dict\n",
    "\n",
    "## Helper function for cross validation\n",
    "def generate_indices(n, K):\n",
    "    ## Split original sample of size n into K sets \n",
    "    indices = np.linspace(0, n, K+1, dtype=int)\n",
    "    return list(zip(indices[:-1], indices[1:]))\n",
    "\n",
    "\n",
    "def train_test_split(input_data, all_inds, kth_test):\n",
    "    \n",
    "    training_ind = [all_inds[i] for i in range(len(all_inds)) if i != kth_test]\n",
    "    test_start, test_end = all_inds[kth_test]\n",
    "    if not tf.is_tensor(input_data):\n",
    "        training_data = np.concatenate([input_data[elm[0]:elm[1]] for elm in training_ind])\n",
    "    else:\n",
    "        \n",
    "        training_data = tf.concat([input_data[elm[0]:elm[1]] for elm in training_ind], axis = 0)\n",
    "    testing_data = input_data[test_start:test_end]\n",
    "    return training_data, testing_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## True Model \n",
    "\n",
    "class MyModel_True_Heterogeneous:\n",
    "    def __init__(self, k, num_treats,promo_ratio):\n",
    "\n",
    "        self.k = k\n",
    "        self.promo_ratio = promo_ratio\n",
    "        self.num_treats = num_treats\n",
    "\n",
    "    def predict(self,inputs):\n",
    "        Q_input = inputs.shape[0]\n",
    "        split_structure =  [1]+ [1] + [1] * self.num_treats + [1]\n",
    "        splitted_elements = tf.split(inputs, split_structure, axis=2)\n",
    "        X_utility =  np.squeeze(np.array(splitted_elements[1]), axis=2)\n",
    "        X_goodbads = np.squeeze(np.array(splitted_elements[0]), axis = 2)\n",
    "\n",
    "        W_matrix =  np.squeeze(np.array(splitted_elements[2]), axis =2 )\n",
    "\n",
    "        final_score_matrix =  W_matrix * self.promo_ratio * X_goodbads + X_utility\n",
    "\n",
    "        ## First element of each row \n",
    "        first_elm = X_utility[:,0]\n",
    "        minus_matrix = first_elm.reshape((len(first_elm),1))@np.ones((1,K))\n",
    "        final_score_matrix_normalized = final_score_matrix - minus_matrix\n",
    "        ## Correct exposure probability \n",
    "        X_logit = np.apply_along_axis(logistic_row, axis=1, arr=final_score_matrix_normalized)\n",
    "    \n",
    "        # expose_indices = np.argmax(X_logit, axis = 1)\n",
    "        # inddds = np.array(list(np.arange(K)) * Q_input).reshape(Q_input,K)\n",
    "        # exposure_matrix = np.array([inddds[i,:] == expose_indices[i] for i in range(Q_input)])\n",
    "\n",
    "        ## Outcome model  \n",
    "        \n",
    "        ## First: a true outcome model of Exponential \n",
    "        outcome_potential = X_utility\n",
    "        pred_out = np.sum(X_logit * outcome_potential, axis = 1 )\n",
    "        pred_out = pred_out.reshape(pred_out.shape[0], 1 )\n",
    "        return np.concatenate([X_logit, pred_out], axis = 1 )\n",
    "    \n",
    "class MyModel_True_Homogeneous:\n",
    "    def __init__(self, k, num_treats,promo_ratio):\n",
    "\n",
    "        self.k = k\n",
    "        self.promo_ratio = promo_ratio\n",
    "        self.num_treats = num_treats\n",
    "\n",
    "    def predict(self,inputs):\n",
    "        Q_input = inputs.shape[0]\n",
    "        split_structure =  [1]+ [1] + [1] * self.num_treats + [1]\n",
    "        splitted_elements = tf.split(inputs, split_structure, axis=2)\n",
    "        X_utility =  np.squeeze(np.array(splitted_elements[1]), axis=2)\n",
    "        X_goodbads = np.squeeze(np.array(splitted_elements[0]), axis = 2)\n",
    "\n",
    "        W_matrix =  np.squeeze(np.array(splitted_elements[2]), axis =2 )\n",
    "\n",
    "        final_score_matrix = W_matrix * self.promo_ratio + X_utility\n",
    "\n",
    "        ## First element of each row \n",
    "        first_elm = X_utility[:,0]\n",
    "        minus_matrix = first_elm.reshape((len(first_elm),1))@np.ones((1,K))\n",
    "        final_score_matrix_normalized = final_score_matrix - minus_matrix\n",
    "        ## Correct exposure probability \n",
    "        X_logit = np.apply_along_axis(logistic_row, axis=1, arr=final_score_matrix_normalized)\n",
    "    \n",
    "        # expose_indices = np.argmax(X_logit, axis = 1)\n",
    "        # inddds = np.array(list(np.arange(K)) * Q_input).reshape(Q_input,K)\n",
    "        # exposure_matrix = np.array([inddds[i,:] == expose_indices[i] for i in range(Q_input)])\n",
    "\n",
    "        ## Outcome model  \n",
    "        \n",
    "        ## First: a true outcome model of Exponential \n",
    "        outcome_potential = X_utility\n",
    "        pred_out = np.sum(X_logit * outcome_potential, axis = 1 )\n",
    "        pred_out = pred_out.reshape(pred_out.shape[0], 1 )\n",
    "        return np.concatenate([X_logit, pred_out], axis = 1 )\n",
    "## True Model \n",
    "\n",
    "# class MyModel_Random:\n",
    "#     def __init__(self, k, num_treats,promo_ratio):\n",
    "\n",
    "#         self.k = k\n",
    "#         self.promo_ratio = promo_ratio\n",
    "#         self.num_treats = num_treats\n",
    "\n",
    "#     def predict(self,inputs):\n",
    "#         output_shape = np.array(input_3d_test_treat.shape)[:2]\n",
    "#         array = np.random.rand(output_shape[0],output_shape[1])\n",
    "#         # Compute the sum of each row\n",
    "#         row_sums = np.sum(array, axis=1)\n",
    "\n",
    "#         # Reshape the row sums to make them compatible for broadcasting\n",
    "#         row_sums = row_sums.reshape(-1, 1)\n",
    "\n",
    "#         normalized_array = array / row_sums\n",
    "\n",
    "#         return normalized_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Data-generating process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of videos \n",
    "J = 30\n",
    "## Consideration set size \n",
    "K = 5\n",
    "k=5\n",
    "## Generate some queries along with the recommendation model \n",
    "Q = 1200\n",
    "## Uplift factor \n",
    "uplift_factor = 1 \n",
    "\n",
    "utility_score_matrix = np.exp(np.random.normal(size=(Q,J)))\n",
    "\n",
    "good_bad_dict = {} \n",
    "treatment_dict = {} \n",
    "utility_score = {} \n",
    "for j in range(J):\n",
    "    good_bad_dict[j] = np.random.choice([True,False], 1)\n",
    "    utility_score[j] = np.random.uniform()\n",
    "X_goodbads = []\n",
    "X_utility = []\n",
    "query_matrix = []\n",
    "for each_query in range(Q):\n",
    "    ## Form the consideration set \n",
    "    selected_indices = np.random.choice(np.arange(J), K, replace= False)\n",
    "    query_matrix += [selected_indices]\n",
    "    X_goodbads = np.append(X_goodbads,[good_bad_dict[ind] for ind in selected_indices])\n",
    "    X_utility = np.append(X_utility, [utility_score_matrix[each_query, ind] for ind in selected_indices])\n",
    "X_goodbads = X_goodbads.reshape(Q, K)\n",
    "X_utility = X_utility.reshape(Q, K)\n",
    "X_utility = X_utility + X_goodbads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the ground truth \n",
    "query_matrix_T,X_goodbads_T,X_utility_T,W_matrix_T, exposure_matrix_T, outcome_potential_T, X_logit_T = DGP_new_heterogeneous(J, Q, K,1, query_matrix, X_goodbads, X_utility,  treat_control_pool = [True, True])\n",
    "query_matrix_C,X_goodbads_C,X_utility_C,W_matrix_C, exposure_matrix_C, outcome_potential_C, X_logit_C= DGP_new_heterogeneous(J, Q, K, 1, query_matrix, X_goodbads, X_utility,  treat_control_pool = [False, False])\n",
    "T_gt = np.sum(X_logit_T * X_utility , axis = 1 )\n",
    "C_gt = np.sum(X_logit_C * X_utility , axis = 1 )\n",
    "\n",
    "ground_truth = np.mean(T_gt) - np.mean(C_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013515599579713822"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Simultation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_B, dim_var_B= [],[]\n",
    "debias_B_true, debias_var_B_true = [],[] \n",
    "undebias_B_true, debias_var_old_B_true = [],[]\n",
    "truth= []\n",
    "undebias_var_B_true = []\n",
    "## Number of iterations of DGP\n",
    "B = 50\n",
    "## Number of videos \n",
    "training_ratio = 0.4\n",
    "JQ_sizes = [(J,Q)]\n",
    "Ks = [5]\n",
    "num_features = 2 \n",
    "\n",
    "\n",
    "\n",
    "## True Outcome Model test \n",
    "L = 1\n",
    "ith_treat = 0\n",
    "\n",
    "## Number of iterations for Hessian matrix estimation \n",
    "M = 100\n",
    "groupNames = [0,1]\n",
    "uplift_ratio = uplift_factor\n",
    "k = K\n",
    "n_folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start K = 5, Q = 1200, J = 30\n",
      "13/13 [==============================] - 0s 406us/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 216\u001b[0m\n\u001b[1;32m    213\u001b[0m W_matrix_m \u001b[39m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(np\u001b[39m.\u001b[39marray(query_matrix)\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    215\u001b[0m     \u001b[39m## Form the consideration set \u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     each_query\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray(query_matrix)[i,:]\n\u001b[1;32m    217\u001b[0m     W_matrix_m \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(W_matrix_m, [[treat_dict_m[ind] \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m each_query]])\n\u001b[1;32m    219\u001b[0m W_matrix_m \u001b[39m=\u001b[39m W_matrix_m\u001b[39m.\u001b[39mreshape(np\u001b[39m.\u001b[39marray(query_matrix)\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for (J, Q) in JQ_sizes:\n",
    "    for K in Ks:\n",
    "        print(\"Start K = {}, Q = {}, J = {}\".format(str(K), str(Q), str(J)))\n",
    "        dim_B, dim_var_B= [],[]\n",
    "        debias_B_true, debias_var_B_true = [],[] \n",
    "        undebias_B_true, debias_var_old_B_true = [],[]\n",
    "\n",
    "        for b in range(B):\n",
    "            query_matrix, X_goodbads, X_utility,W_matrix, exposure_matrix, outcome_potential, X_logit = DGP_new_heterogeneous(J, Q, K, uplift_ratio, query_matrix, X_goodbads, X_utility, treat_control_pool = [True, False])\n",
    "            \n",
    "            ## Cross-fitting indices \n",
    "            all_inds = generate_indices(np.array(query_matrix).shape[0], n_folds)\n",
    "\n",
    "            ## Iterate over each fold for cross-validation. \n",
    "            hfuncs_each_fold,  debias_terms_each_fold = {},{}\n",
    "\n",
    "            for f in range(n_folds):\n",
    "                f_start, f_end = all_inds[f]\n",
    "                \n",
    "                ## Train-test split \n",
    "                query_train, query_test = train_test_split(np.array(query_matrix), all_inds, f)\n",
    "                X_goodbads_train, X_goodbads_test =  train_test_split(X_goodbads, all_inds, f) \n",
    "                X_utility_train, X_utility_test =  train_test_split(X_utility, all_inds, f)  \n",
    "                W_matrix_train, W_matrix_test = train_test_split(W_matrix, all_inds, f) \n",
    "                observed_queries_treatment = np.sum(exposure_matrix * W_matrix, axis = 1 )\n",
    "                observed_outcome = np.sum(outcome_potential * exposure_matrix, axis = 1 )\n",
    "\n",
    "                T, C = observed_outcome[observed_queries_treatment == groupNames[ith_treat + 1 ]] , observed_outcome[observed_queries_treatment == 0]  \n",
    "                exposure_matrix_train,exposure_matrix_test =train_test_split(exposure_matrix, all_inds, f) \n",
    "                outcome_matrix = exposure_matrix * outcome_potential\n",
    "                outcome_matrix = np.sum(outcome_matrix, axis = 1 ).reshape(outcome_matrix.shape[0],1)\n",
    "\n",
    "                observed_outcome_train, observed_outcome_test = train_test_split(observed_outcome, all_inds, f) \n",
    "                outcome_matrix_train, outcome_matrix_test = train_test_split(outcome_matrix, all_inds, f) \n",
    "                outcome_potential_train, outcome_potential_test = train_test_split(outcome_potential, all_inds, f)  \n",
    "                inputs_3d_train = tf.stack([X_goodbads_train,X_utility_train, W_matrix_train, X_utility_train ], axis = -1)\n",
    "                inputs_3d_test = tf.stack([X_goodbads_test,X_utility_test, W_matrix_test, X_utility_test], axis = -1)\n",
    "\n",
    "                input_3d_test_treat = tf.stack([X_goodbads_test,X_utility_test, np.ones(W_matrix_test.shape), X_utility_test], axis = -1)\n",
    "                input_3d_test_control = tf.stack([X_goodbads_test,X_utility_test, np.zeros(W_matrix_test.shape), X_utility_test], axis = -1)\n",
    "                output_3d_train = tf.concat([tf.cast(exposure_matrix_train, dtype=float),outcome_matrix_train], axis = 1)\n",
    "                output_3d_test = tf.concat([tf.cast(exposure_matrix_test, dtype=float),outcome_matrix_test ], axis = 1)\n",
    "\n",
    "                exposure_indicator_array = exposure_matrix_test\n",
    "\n",
    "                ## Get treatment indicator matrix\n",
    "                w_dict = {}\n",
    "\n",
    "                for l in range(L):\n",
    "                    w_dict[l] = tf.convert_to_tensor(W_matrix == groupNames[l+1], dtype = float)\n",
    "\n",
    "                # training_num = int(W_matrix.shape[0] * training_ratio)\n",
    "                # testing_num = W_matrix.shape[0] - training_num\n",
    "\n",
    "\n",
    "                w_all_treat = tf.convert_to_tensor(np.array([[ith_treat + 1] * k for _ in range(W_matrix.shape[0])],dtype='float32'))\n",
    "                w_all_control = tf.convert_to_tensor(np.array([[0] * k for _ in range(W_matrix.shape[0])],dtype='float32'))\n",
    "\n",
    "                inputs_all_treat_3d = tf.stack([X_goodbads,X_utility] + [w_all_treat if l == ith_treat else w_all_control for l in range(L)] +[ X_utility], axis = 2)\n",
    "                inputs_all_control_3d = tf.stack([X_goodbads,X_utility] + [w_all_control if l == ith_treat else w_all_control for l in range(L)] +[ X_utility ], axis = 2)\n",
    "                inputs_all_treat_3d = tf.cast(inputs_all_treat_3d, dtype = 'float32')\n",
    "                inputs_all_control_3d = tf.cast(inputs_all_control_3d, dtype = 'float32')\n",
    "\n",
    "\n",
    "                ## All other all_treated \n",
    "                inputs_all_treat_3d_dict = {} \n",
    "                for l in range(L):\n",
    "                    inputs_all_treat_3d_l = tf.stack([X_goodbads,X_utility] + [w_all_treat if l == v else w_all_control for v in range(L)] +[ X_utility ], axis = 2)\n",
    "                    #inputs_all_treat_3d_l = tf.stack([x_basebid, x_sort_score, x_bid,x_ecpm, x_cvr] + [w_all_treat if v == l else w_all_control for v in range(L)] + [x_cvr], axis = 2)\n",
    "                    inputs_all_treat_3d_dict[l] = tf.cast(inputs_all_treat_3d_l, dtype = 'float32')\n",
    "\n",
    "                exposure_indicator_outcome_train, exposure_indicator_outcome_test = outcome_matrix_train, outcome_matrix_test\n",
    "                inputs_all_treat_3d_test = input_3d_test_treat\n",
    "                inputs_all_control_3d_test = input_3d_test_control\n",
    "                is_selected_indicator_train,is_selected_indicator_test = exposure_matrix_train,exposure_matrix_test\n",
    "\n",
    "                treat_control_dict = {} \n",
    "                for l in range(L):\n",
    "\n",
    "                    \n",
    "                    inputs_3d_train_l,inputs_3d_test_l= train_test_split(inputs_all_treat_3d_dict[l], all_inds, f) \n",
    "\n",
    "                    treat_control_dict[l] = {'train':inputs_3d_train_l, 'test': inputs_3d_test_l}\n",
    "\n",
    "                myModelMultiple = MyModel_True_Heterogeneous(K, L, uplift_ratio)\n",
    "                # myModelMultiple_random = MyModel_Random(K, L, uplift_ratio)\n",
    "                # myModelMultiple.compile(loss=custom_loss,optimizer=tf.keras.optimizers.legacy.Adam())\n",
    "                # myModelMultiple.fit(input_3d_train,output_3d_train , epochs=10, verbose=False)\n",
    "                exposure_indicator_array = is_selected_indicator_test\n",
    "                treatment_indicator_array = 1 * (np.array(w_dict[ith_treat])[f_start:f_end,:])\n",
    "                res_tempt = np.array(myModelMultiple.predict(inputs_all_treat_3d_test)) - np.array(myModelMultiple.predict(inputs_all_control_3d_test))\n",
    "\n",
    "\n",
    "                # pred_H_new = np.array(myModelMultiple.predict(inputs_all_treat_3d_test)) - np.array(myModelMultiple.predict(inputs_all_control_3d_test))\n",
    "                model_pred_H = np.array(myModelMultiple.predict(inputs_3d_test))\n",
    "                model_pred_all_treat = myModelMultiple.predict(inputs_all_treat_3d_test)\n",
    "                model_pred_all_control = myModelMultiple.predict(inputs_all_control_3d_test)\n",
    "                all_treat_array, all_control_array = np.array(model_pred_all_treat), np.array(model_pred_all_control)\n",
    "\n",
    "                ## All other counterfactuals \n",
    "                counterfactual_pred_dict = {} \n",
    "                for l in range(L):\n",
    "                    counterfactual_pred_dict[l] = myModelMultiple.predict(treat_control_dict[l]['test'])\n",
    "\n",
    "                ## Outcome - prediction model \n",
    "                indicator_bool = tf.cast(is_selected_indicator_train, dtype=tf.bool)\n",
    "                selected_elements = tf.boolean_mask(inputs_3d_train[:,:,:num_features], indicator_bool)\n",
    "\n",
    "                input_to_outcomemodel_train = tf.reshape(selected_elements, (inputs_3d_train.shape[0], num_features))\n",
    "                # Define your base model\n",
    "                base_model = tf.keras.Sequential()\n",
    "                base_model.add(layers.Dense(1, input_shape=(num_features,)))\n",
    "\n",
    "                # Compile the model\n",
    "                base_model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "                base_model.fit(input_to_outcomemodel_train,output_3d_train[:, K],epochs=50, verbose=False)\n",
    "                # Now define a new model for prediction\n",
    "                model_for_prediction = tf.keras.Sequential()\n",
    "                model_for_prediction.add(layers.TimeDistributed(base_model, input_shape=(K, num_features)))\n",
    "                predictions = model_for_prediction.predict(inputs_3d_test[:,:,:num_features])\n",
    "                # Remove the third dimension of size 1\n",
    "                numpy_array_pred = np.squeeze(predictions, axis=2)\n",
    "\n",
    "                mus_T, mus_C  = numpy_array_pred,numpy_array_pred\n",
    "                p_T, p_C  = all_treat_array[:, :k], all_control_array[:,:k]\n",
    "                rewards_array = observed_outcome_test\n",
    "                rewards_array = rewards_array.reshape(rewards_array.shape[0],1)\n",
    "                Ey1,Ey0 = np.sum(mus_T * p_T, axis = 1), np.sum(mus_C * p_C, axis = 1)\n",
    "                pv1,pv0 = np.sum(exposure_indicator_array * p_T, axis = 1), np.sum(exposure_indicator_array * p_C, axis = 1)\n",
    "\n",
    "                pv_given_uvw = p_T * treatment_indicator_array + p_C * (1 - treatment_indicator_array)\n",
    "\n",
    "\n",
    "                p_realized = model_pred_H[:,:K]\n",
    "\n",
    "\n",
    "\n",
    "                ## 1. COMPUTE THE GRADIENT OF LOSSS  \n",
    "                ## FIX: change to realized outcome \n",
    "                #dl1dtheta0 = pv_given_uvw - exposure_indicator_array\n",
    "                dl1dtheta0 = p_realized - exposure_indicator_array\n",
    "                dl1dtheta0 = dl1dtheta0[:, 1:] \n",
    "\n",
    "\n",
    "                ## FIX: iterate over all L \n",
    "                dl1dthetal_dict = {} \n",
    "                for l in range(L):\n",
    "                    treatment_indicator_array_l = w_dict[l][f_start:f_end, :]\n",
    "                    dl1dthetal_dict[l] = treatment_indicator_array_l *  (p_realized - exposure_indicator_array)\n",
    "                dl2dmu = exposure_indicator_array * (mus_T -rewards_array)\n",
    "                gradient_vector_l = np.concatenate([dl1dtheta0]+[dl1dthetal_dict[l] for l in range(L)] +[dl2dmu], axis =1 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                ## 2. COMPUTE  THE GRADIENT OF H FUNCTION\n",
    "                dHdtheta0 = p_T * (mus_T - Ey1.reshape(mus_T.shape[0],1)) - p_C * (mus_C - Ey0.reshape(mus_C.shape[0],1))\n",
    "                dHdtheta0 = dHdtheta0[:, 1:]\n",
    "\n",
    "\n",
    "\n",
    "                ## FIX: iterate over each l \n",
    "                dHdthetal_dict = {} \n",
    "                for l in range(L):\n",
    "\n",
    "                    p_T_thetal = counterfactual_pred_dict[l][:,:k]\n",
    "                    Eyl = np.sum(mus_T * p_T_thetal, axis = 1)\n",
    "                    dHdthetal_dict[l] = p_T_thetal * (mus_T - Eyl.reshape(mus_T.shape[0],1))\n",
    "                    ## 0 for the groups that are not the target treatment group \n",
    "                    if l != ith_treat:\n",
    "                        dHdthetal_dict[l] = 0 * (p_T_thetal * (mus_T - Eyl.reshape(mus_T.shape[0],1)))\n",
    "\n",
    "                #dHdthetal = p_T * (mus_T - Ey1.reshape(mus_T.shape[0],1))\n",
    "                dHdmu = p_T - p_C\n",
    "                #gradient_vector_H = np.concatenate([dHdtheta0,dHdthetal,dHdmu], axis =1 )\n",
    "\n",
    "                ## FIX: iterate over all l \n",
    "                gradient_vector_H = np.concatenate([dHdtheta0]+[dHdthetal_dict[l] for l in range(L)]+[dHdmu], axis =1 )\n",
    "                ## Gradient over all other treatments \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                ## 3. FIND THE EXPECTATION OF HESSIAN MATRIX \n",
    "\n",
    "\n",
    "\n",
    "                Hessian_all = np.zeros((inputs_3d_test.shape[0],(L+2) * K - 1,  (L+2) * K - 1))\n",
    "\n",
    "                montecarlo_expected_probability = np.zeros(exposure_indicator_array.shape)\n",
    "\n",
    "                selected_indicator_dict  = {}\n",
    "                assignment_pd_dict = {} \n",
    "                dmu_dict = {} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # for m in range(M):\n",
    "                #     treat_dict_m = permute_treatment_dict(J)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                M = 500\n",
    "                for m in range(M):\n",
    "                    w_dict_m = {} \n",
    "                    treat_dict_m = permute_treatment_dict(J, L + 1)\n",
    "                    W_matrix_m = []\n",
    "                    for i in range(np.array(query_matrix).shape[0]):\n",
    "                        ## Form the consideration set \n",
    "                        each_query=np.array(query_matrix)[i,:]\n",
    "                        W_matrix_m = np.append(W_matrix_m, [[treat_dict_m[ind] for ind in each_query]])\n",
    "\n",
    "                    W_matrix_m = W_matrix_m.reshape(np.array(query_matrix).shape)\n",
    "\n",
    "\n",
    "\n",
    "                    for l in range(L):\n",
    "                        w_dict_m[l] = tf.convert_to_tensor(W_matrix_m == groupNames[l + 1], dtype = float)\n",
    "\n",
    "\n",
    "                    inputs_3d_m = tf.stack([X_goodbads,X_utility]+  [w_dict_m[l] for l in range(L)] +[X_goodbads], axis = -1)\n",
    "                    inputs_3d_test_m = inputs_3d_m[f_start:f_end,:]\n",
    "                    model_pred_m = np.array(myModelMultiple.predict(inputs_3d_test_m))[:,:k]\n",
    "                    outer_product_pv1pv2 = np.array([np.outer(row_[1:], row_[1:]) for row_ in model_pred_m])\n",
    "                    outer_product_treatment_indicator = np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                    outer_product_pv1_one_minus_pv2 = np.array([np.outer(row_, 1-row_) for row_ in model_pred_m])\n",
    "                    # p_treat = 1/(L+1) \n",
    "\n",
    "                    is_selected_indicator_test = np.array(exposure_matrix[f_start:f_end,:])\n",
    "                    selected_indicator_dict[m] = is_selected_indicator_test \n",
    "                    d2l2dtheta0 = - np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "\n",
    "                    ## FIX: Iterate over l \n",
    "                    d2l2dthetal_dict = {}\n",
    "                    for l in range(L):\n",
    "                        ## K by K \n",
    "\n",
    "                        w_m_l = np.array(w_dict_m[l][f_start:f_end,:])\n",
    "\n",
    "                        ## Off-diagonal terms \n",
    "                        d2l2dtheta1 =  np.array([np.outer(row_, row_) for row_ in w_m_l]) * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                        # d2l2dtheta1 = - p_treat * p_treat * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                        ## Modify diagonal terms\n",
    "                        for i in range(d2l2dtheta1.shape[0]):\n",
    "                            treat_indicator_i = w_m_l[i,:]\n",
    "                            probs_i = model_pred_m[i,:]\n",
    "                            np.fill_diagonal(d2l2dtheta1[i],treat_indicator_i * probs_i * (1-probs_i))\n",
    "                            # np.fill_diagonal(d2l2dtheta1[i], p_treat * probs_i * (1-probs_i))\n",
    "                        d2l2dthetal_dict[l] = d2l2dtheta1\n",
    "\n",
    "\n",
    "\n",
    "                    ## FIX: iterate over all l1, l2 \n",
    "                    d2ldthetal1dthetal2 = {} \n",
    "                    for l in range(L):\n",
    "                        w_m_l = np.array(w_dict_m[l][f_start:f_end,:])\n",
    "                        ## Off-diagonal terms \n",
    "                        ## !!!!!!!!!!!!!!!!! Ruohan: please double check the following, I modified the original one.\n",
    "                        d2l2dtheta0dtheta1 = - np.multiply(w_m_l[:,np.newaxis, :], np.array([np.outer(row_, row_) for row_ in model_pred_m]))\n",
    "                        # d2l2dtheta0dtheta1 = - p_treat * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                        for i in range(d2l2dtheta0dtheta1.shape[0]):\n",
    "                            treat_indicator_i = w_m_l[i,:]\n",
    "                            p_1minusp_i = model_pred_m[i,:] * (1 - model_pred_m[i,:])\n",
    "                            np.fill_diagonal(d2l2dtheta0dtheta1[i], treat_indicator_i * p_1minusp_i)\n",
    "                            # np.fill_diagonal(d2l2dtheta0dtheta1[i], p_treat * p_1minusp_i)\n",
    "\n",
    "                        ## NOTE: -1 to indicate the baseline theta \n",
    "                        d2ldthetal1dthetal2[(-1,l)] = d2l2dtheta0dtheta1[:,1:,:]\n",
    "                        d2ldthetal1dthetal2[(l,-1)] = np.transpose(d2l2dtheta0dtheta1[:,1:,:], (0,2,1))\n",
    "                        for l_prime in range(L):\n",
    "                            if l != l_prime: \n",
    "                                w_m_l = np.array(w_dict_m[l][training_num:,:])\n",
    "                                w_m_l_prime = np.array(w_dict_m[l_prime][training_num:,:])\n",
    "                                indicator_outer = np.array([np.outer(w_m_l[i,:], w_m_l_prime[i,:]) for i in range(w_m_l.shape[0])])\n",
    "\n",
    "                                d2l2dthetal1dthetal2 = -  indicator_outer * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                                # d2l2dthetal1dthetal2 = -  p_treat * p_treat * np.array([np.outer(row_, row_) for row_ in model_pred_m])\n",
    "                                d2ldthetal1dthetal2[(l,l_prime)]  = d2l2dthetal1dthetal2\n",
    "                                d2ldthetal1dthetal2[(l_prime,l)]  = np.transpose(d2l2dthetal1dthetal2, (0,2,1))\n",
    "                            else:\n",
    "                                d2ldthetal1dthetal2[(l,l)] = d2l2dthetal_dict[l]\n",
    "\n",
    "\n",
    "                    d2l2dmu = np.zeros(d2l2dtheta1.shape)\n",
    "\n",
    "                    # treatment_indicator_array_m = \n",
    "                    for i in range(d2l2dmu.shape[0]):\n",
    "                        #p_1minusp_i = (1 - model_pred_m[i,:]) * (1 - model_pred_m[i,:])\n",
    "                        p_1minusp_i = model_pred_m[i,:] * (1 - model_pred_m[i,:])\n",
    "                        # treatment_i = treatment_indicator_array[i,:]\n",
    "                        # exposure_i = is_selected_indicator_test[i,:]\n",
    "                        np.fill_diagonal(d2l2dtheta0[i], p_1minusp_i)\n",
    "                        np.fill_diagonal(d2l2dmu[i], model_pred_m[i,:])\n",
    "\n",
    "\n",
    "\n",
    "                    d2l2dtheta0 = d2l2dtheta0[:,1:, 1:]\n",
    "                    # d2l2dtheta01_k_m_1_by_k = d2l2dtheta01[:, 1:,:]\n",
    "                    # d2l2dtheta10_k_by_k_m_1 = d2l2dtheta01[:, :,1:]\n",
    "                    Hessian_first_row = np.concatenate([d2l2dtheta0] + [d2ldthetal1dthetal2[(-1, l)] for l in range(L)] + [np.zeros((d2l2dtheta0.shape[0], K-1, K))], axis =2)\n",
    "\n",
    "                    ## 1 to L + 1 row \n",
    "                    Hessian_middle_dict = {}\n",
    "                    for l in range(L):\n",
    "                        row_l = np.concatenate([d2ldthetal1dthetal2[(l, -1)]] + [d2ldthetal1dthetal2[(l, l_prime)] for l_prime in range(L)] +[np.zeros((d2l2dtheta0.shape[0], K, K))], axis =2)\n",
    "\n",
    "                        Hessian_middle_dict[l] = row_l                                                                           \n",
    "\n",
    "\n",
    "                    Hessian_third_row = np.concatenate((np.zeros((d2l2dtheta0.shape[0], K, K  * (L + 1 ) - 1 )), d2l2dmu), axis =2)\n",
    "\n",
    "                    Hessian = np.concatenate([Hessian_first_row] + [Hessian_middle_dict[l] for l in range(L)] + [Hessian_third_row], axis = 1 )\n",
    "\n",
    "                    dmu_dict[m] = d2l2dmu\n",
    "\n",
    "                    Hessian_all = Hessian_all + Hessian\n",
    "\n",
    "                Hessian_final = Hessian_all / M\n",
    "                count_finite = 0\n",
    "                score_funcs = np.zeros(len(Hessian_final))\n",
    "                for i in range(len(Hessian_final)):\n",
    "                    if is_invertible(Hessian_final[i]):\n",
    "                        try:\n",
    "                            score_funcs[i] = gradient_vector_H[i]@np.linalg.inv(Hessian_final[i])@gradient_vector_l[i]\n",
    "                            count_finite += 1 \n",
    "                        except: \n",
    "                            print(\"Fail for inversion\")\n",
    "                outs_1 = res_tempt[score_funcs !=0,K]\n",
    "\n",
    "\n",
    "                ## END OF FOR LOOP FOR EACH ITERATION OVER CROSS FITTING\n",
    "                hfuncs_f, debias_term_f = Ey1 - Ey0,score_funcs\n",
    "                # debias_point_f,  debias_var_f, undebiased_point_f  = debias_estimator(outs_1, score_funcs[score_funcs!=0])\n",
    "                # debias_point_each_fold += [debias_point_f]  \n",
    "                # debias_var_each_fold += [debias_var_f]\n",
    "                # undebiased_point_each_fold += [undebiased_point_f]\n",
    "                hfuncs_each_fold[f] =hfuncs_f\n",
    "                debias_terms_each_fold[f] = debias_term_f\n",
    "\n",
    "            tau_hat_undebias = np.mean([ np.mean(hfuncs_each_fold[f])for f in range(n_folds)])\n",
    "            tau_hat_debias = np.mean([ np.mean(hfuncs_each_fold[f] - debias_terms_each_fold[f])  for f in range(n_folds)])\n",
    "            debias_point = tau_hat_debias\n",
    "            debias_var = np.mean([debias_estimator_new(hfuncs_each_fold[f] ,debias_terms_each_fold[f], tau_hat_debias)[1] for f in range(n_folds)])\n",
    "            undebias_var = np.mean([undebias_estimator_new(hfuncs_each_fold[f] ,tau_hat_undebias)[1] for f in range(n_folds)])\n",
    "            undebias_point = tau_hat_undebias\n",
    "            dim_point, dim_var = dim_est(T, C)\n",
    "            dim_B += [dim_point]\n",
    "            dim_var_B += [dim_var]\n",
    "            debias_B_true += [debias_point]\n",
    "            debias_var_B_true += [debias_var]\n",
    "            undebias_B_true += [undebias_point]\n",
    "            undebias_var_B_true += [undebias_var]\n",
    "\n",
    "        result_df = pd.DataFrame({\"debias_point\": debias_B_true, \"debias_var\":debias_var_B_true, \"dim\": dim_B, \n",
    "                                 \"dim_var\":dim_var_B, \"undebias_point\": undebias_B_true, \"undebias_var\": undebias_var_B_true, \"J\" : J,\"Q\": Q, \"K\":K })\n",
    "        result_df.to_csv(\"result2405new/new_heterogeneous_{}_synthetic_ab_j{}q{}k{}_100_{}.csv\".format(str(int(time.time())),str(J), str(Q), str(K), str(uplift_ratio).replace('.','')))\n",
    "        plt.figure() \n",
    "        sns.kdeplot(np.array(debias_B_true) /  np.sqrt(np.array(debias_var_B_true)/(int(Q))) , shade = True,color=tencent_blue,label = \"Ours(debiased)\",alpha=0.1)\n",
    "        sns.kdeplot(np.array(undebias_B_true) /  np.sqrt(np.array(undebias_var_B_true)/(int(Q))) , shade = True,color='red',label = \"Ours(undebiased)\",alpha=0.1)\n",
    "        sns.kdeplot(np.array(dim_B) / np.sqrt(np.array(dim_var_B)), shade = True,color=tencent_orange,label = \"DIM\",alpha=0.1)\n",
    "        plt.plot(x, y_standard_normal, color='black', label=\"Standard Normal\", ls='--')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.savefig(\"result2406new/new_heterogeneous_{}_synthetic_ab_j{}q{}k{}_density{}.png\".format(str(int(time.time())), str(J), str(Q), str(K), str(uplift_ratio).replace('.','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tempt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
